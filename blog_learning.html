<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Part 1: Learning</title>
    <meta name="theme-color" content="#0A0A0F"/>
    <meta name="description" content="Neural networks are the engines driving much of modern artificial intelligence. This tutorial walks you through building and training a simple neural network from the ground up, using plain JavaScript." />
    <style>
        :root{
            --bg:#0a0a0f;
            --bg-2:#0e0e16;
            --panel:#12121c;
            --panel-2:#171726;
            --text:#e9e9f2;
            --muted:#a5a7bf;
            --line:#1e1e2e;
            --pink:#ff2e88;
            --pink-2:#ff86c3;
            --glow: 0 0 .5rem var(--pink), 0 0 1.25rem color-mix(in oklab, var(--pink) 70%, white 0%), 0 0 2.5rem color-mix(in oklab, var(--pink) 35%, black 65%);
            --radius:16px;
            --pad: clamp(14px, 1.6vw, 22px);
            --font: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", "Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";
            --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        *{box-sizing:border-box}
        html,body{min-height:100%}
        body{
            margin:0;
            font-family:var(--font);
            color:var(--text);
            background:
                radial-gradient(1200px 520px at 50% -160px, rgba(255,46,136,.12), transparent 75%),
                linear-gradient(180deg, #090914 0%, #050509 45%, #030306 75%, #020203 100%);
            background-color:#020203;
            background-repeat:no-repeat, no-repeat;
            background-size:130% 70%, 100% 100%;
            background-position:center top, center top;
            background-attachment: fixed;
            line-height:1.6;
            overflow-x:hidden;
            -webkit-font-smoothing:antialiased;
            -moz-osx-font-smoothing:grayscale;
        }

        /* Subtle scanlines for retro vibe */
        body::before{
            content:"";
            position:fixed; inset:0;
            pointer-events:none;
            background-image: linear-gradient(rgba(255,255,255,.03),rgba(255,255,255,0) 2px);
            background-size:100% 3px; mix-blend-mode:overlay; opacity:.25;
        }
        @media (prefers-reduced-motion: reduce){
            body::before{display:none}
            *{animation: none !important; transition: none !important}
        }

        .container{width:min(1000px, 92vw); margin-inline:auto}

        /* Header */
        header{
            background: linear-gradient(180deg, #131321, #11111c);
            border-bottom: 1px solid var(--line);
            padding: 1.5rem 0;
            position: sticky;
            top: 0;
            z-index: 10;
            backdrop-filter: blur(10px);
        }

        nav{
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 1rem;
        }

        .logo{
            font-weight: 900;
            font-size: 1.25rem;
            color: var(--text);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .logo .pink{
            color: var(--pink);
            text-shadow: var(--glow);
        }

        .back-btn{
            appearance:none;
            border:1px solid color-mix(in oklab, var(--pink) 35%, #ffffff00 65%);
            color:var(--text);
            background: linear-gradient(180deg, var(--panel) 0%, var(--panel-2) 100%);
            padding:.45rem .85rem;
            border-radius: 999px;
            cursor:pointer;
            font-weight:600;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 0.35rem;
            font-size: 0.9rem;
            transition: transform .12s ease, box-shadow .15s ease;
        }

        .back-btn:hover{
            box-shadow: var(--glow);
            transform: translateY(-1px);
        }

        /* Article Hero */
        .article-hero{
            padding: 3rem 0 2rem;
            position: relative;
        }

        .grid-bg{
            position:absolute; inset:0; pointer-events:none; z-index:-1; opacity:.25;
            background:
                radial-gradient(circle at 50% -60px, color-mix(in oklab, var(--pink) 30%, transparent), transparent 35%),
                repeating-linear-gradient(0deg, #ffffff10 0 1px, transparent 1px 36px),
                repeating-linear-gradient(90deg, #ffffff10 0 1px, transparent 1px 36px);
            mask: radial-gradient(1200px 600px at 50% 0, black 40%, transparent 80%);
        }

        h1{
            font-size: clamp(32px, 5vw, 48px);
            line-height: 1.1;
            font-weight: 900;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--text) 0%, var(--pink) 100%);
            -webkit-background-clip: text;
            background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .article-meta{
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            color: var(--muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }

        .article-meta span{
            display: flex;
            align-items: center;
            gap: 0.35rem;
        }

        .article-blurb{
            font-size: 1.125rem;
            line-height: 1.6;
            color: #d9d9ec;
            max-width: 800px;
        }

        /* Article Content */
        article{
            background: linear-gradient(180deg, #131321, #11111c);
            border: 1px solid #222238;
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 2rem 0 3rem;
            box-shadow: 0 18px 50px rgba(0,0,0,.45), 0 0 60px -10px rgba(255,46,136,.15);
        }

        article h1{
            color: var(--text);
            margin: 2.5rem 0 1rem 0;
            font-size: 2.25rem;
            font-weight: 900;
            position: relative;
            padding-bottom: 0.75rem;
            background: none;
            -webkit-text-fill-color: var(--text);
        }

        article h1::after{
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 80px;
            height: 3px;
            background: linear-gradient(90deg, var(--pink), transparent);
        }

        article h1:first-child{
            margin-top: 0;
        }

        article h2{
            color: var(--text);
            margin: 2.5rem 0 1rem 0;
            font-size: 1.875rem;
            font-weight: 800;
            position: relative;
            padding-bottom: 0.75rem;
        }

        article h2::after{
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 2px;
            background: linear-gradient(90deg, var(--pink), transparent);
        }

        article h2:first-child{
            margin-top: 0;
        }

        article h3{
            color: var(--text);
            margin: 2rem 0 1rem 0;
            font-size: 1.375rem;
            font-weight: 700;
        }

        article h4{
            color: var(--text);
            margin: 1.5rem 0 0.75rem 0;
            font-size: 1.125rem;
            font-weight: 600;
        }

        article p{
            margin-bottom: 1.25rem;
            color: #d9d9ec;
            line-height: 1.7;
        }

        article ul, article ol{
            margin: 1.25rem 0;
            padding-left: 1.75rem;
            color: #d9d9ec;
        }

        article li{
            margin-bottom: 0.5rem;
            line-height: 1.7;
        }

        article strong{
            color: var(--text);
            font-weight: 600;
        }

        /* Links within article content */
        article a{
            color: var(--pink-2);
            text-decoration: none;
            border-bottom: 1px solid rgba(255, 134, 195, 0.3);
            transition: all 0.2s ease;
            font-weight: 500;
        }

        article a:hover{
            color: var(--pink);
            border-bottom-color: var(--pink);
            text-shadow: 0 0 8px rgba(255, 46, 136, 0.4);
        }

        article a:visited{
            color: color-mix(in oklab, var(--pink-2) 80%, var(--muted) 20%);
        }

        /* Code blocks */
        pre{
            background: #0f0f1b;
            border: 1px solid #2a2a3a;
            color: #e2e8f0;
            padding: 1.25rem;
            border-radius: 12px;
            margin: 1.5rem 0;
            overflow-x: auto;
            font-family: var(--mono);
            font-size: 0.875rem;
            line-height: 1.6;
            box-shadow: inset 0 2px 8px rgba(0,0,0,.3);
        }

        code{
            font-family: var(--mono);
            font-size: 0.9em;
            background: rgba(255, 46, 136, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            color: var(--pink-2);
        }

        pre code{
            background: none;
            padding: 0;
            color: #e2e8f0;
        }

        /* JavaScript Syntax Highlighting */
        .language-javascript .keyword { color: #ff79c6; font-weight: 600; }
        .language-javascript .function { color: #8be9fd; }
        .language-javascript .string { color: #f1fa8c; }
        .language-javascript .number { color: #bd93f9; }
        .language-javascript .comment { color: #6272a4; font-style: italic; }
        .language-javascript .operator { color: #ff79c6; }
        .language-javascript .class-name { color: #50fa7b; }
        .language-javascript .constant { color: #ff79c6; }
        .language-javascript .punctuation { color: #a5a7bf; }
        .language-javascript .boolean { color: #bd93f9; font-weight: 600; }
        .language-javascript .null { color: #bd93f9; font-weight: 600; }
        .language-javascript .undefined { color: #bd93f9; font-weight: 600; }

        /* Neuron diagram container */
        .neuron-diagram-container {
            background: #0a0a14;
            border: 1px solid color-mix(in oklab, var(--pink) 25%, #2a2a3a 75%);
            border-radius: 12px;
            padding: 1rem;
            margin: 1.5rem 0;
            display: flex;
            justify-content: center;
            align-items: center;
            box-shadow: inset 0 2px 8px rgba(0,0,0,.3);
        }

        #neuronDiagram {
            max-width: 100%;
            height: auto;
        }

        /* Tables */
        table{
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: var(--panel);
            border: 1px solid #2a2a3a;
            border-radius: 8px;
            overflow: hidden;
        }

        th{
            background: linear-gradient(180deg, color-mix(in oklab, var(--pink) 15%, var(--panel) 85%), var(--panel-2));
            color: var(--text);
            padding: 0.75rem;
            text-align: left;
            font-weight: 600;
            border-bottom: 1px solid color-mix(in oklab, var(--pink) 30%, #2a2a3a 70%);
        }

        td{
            padding: 0.75rem;
            border-bottom: 1px solid #2a2a3a;
            color: #d9d9ec;
        }

        tr:last-child td{
            border-bottom: none;
        }

        tr:hover{
            background: rgba(255, 46, 136, 0.05);
        }

        /* Blockquotes */
        blockquote{
            border-left: 3px solid var(--pink);
            background: #0a0a14;
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            border-radius: 4px;
            font-style: italic;
            color: #d9d9ec;
        }

        blockquote p{
            margin-bottom: 0.5rem;
            margin-top: 0;
        }

        blockquote p:last-child{
            margin-bottom: 0;
        }

        /* Navigation */
        .article-nav{
            display: flex;
            justify-content: space-between;
            gap: 1rem;
            margin: 3rem 0;
            padding-top: 2rem;
            border-top: 1px solid var(--line);
        }

        .nav-link{
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--pink-2);
            text-decoration: none;
            font-weight: 600;
            transition: transform 0.2s;
        }

        .nav-link:hover{
            color: var(--pink);
            transform: translateX(3px);
        }

        .nav-link.prev:hover{
            transform: translateX(-3px);
        }

        /* Footer */
        footer{
            border-top: 1px solid var(--line);
            background: #0a0a12;
            color: var(--muted);
            padding: 1rem 0 1.5rem;
            font-size: 0.875rem;
            margin-top: 4rem;
        }

        footer a{
            color: var(--pink-2);
            text-decoration: none;
            border-bottom: 1px solid rgba(255, 134, 195, 0.3);
            transition: all 0.2s ease;
        }

        footer a:hover{
            color: var(--pink);
            border-bottom-color: var(--pink);
            text-shadow: 0 0 8px rgba(255, 46, 136, 0.4);
        }

        /* Responsive */
        @media (max-width: 768px){
            article{
                padding: 1.5rem;
            }

            h1{
                font-size: 2rem;
            }

            .article-meta{
                font-size: 0.8rem;
                gap: 1rem;
            }

            pre{
                padding: 1rem;
                font-size: 0.75rem;
            }

            .neuron-diagram-container {
                padding: 1rem;
            }

            #neuronDiagram {
                width: 100%;
                height: auto;
            }
        }
    </style>
</head>
<body>
    <!-- Header/Navigation -->
    <header>
        <nav class="container">
            <a href="index.html" class="logo">
                The Gift of <span class="pink">Gab</span>
            </a>
            <a href="index.html" class="back-btn">
                Back to Home
            </a>
        </nav>
    </header>

    <!-- Main Article Content -->
    <main class="container">
        <article>
            <h1>Part 1: Learning</h1>

            <p>
                Neural networks are the engines driving much of modern artificial intelligence. They power recommendation systems, image generators, and language models. While libraries like TensorFlow and PyTorch make it easy to build complex networks, understanding the underlying mechanics is important for genuine insight.
            </p>
            <p>
                This tutorial walks you through building and training a simple neural network from the ground up, using plain JavaScript. We'll implement the neural network step by step, explaining the <em>why</em> behind the <em>how</em> without getting lost in the math of it all.
            </p>

            <h1>A Trainable Neuron</h1>

            <p>
                At the heart of every neural network lies the neuron, a simple computational unit. A neuron takes multiple inputs (for now an array of floating point numbers), performs a weighted sum of those inputs, and adds a bias term to produce output. It's a surprisingly powerful primitive.
            </p>

            <div class="neuron-diagram-container">
                <canvas id="neuronDiagram" width="600" height="200"></canvas>
            </div>

            <blockquote>
                <p>A neuron with two inputs, <code>Input0</code> and <code>Input1</code>. This neuron holds two weights, <code>Weight0</code> and <code>Weight1</code>, and a bias term, <code>Bias</code>. The output is calculated as: <code>Output = (Input0 * Weight0 + Input1 * Weight1) + Bias</code>.</p>
            </blockquote>

            <p>Let's translate this into code. We'll start with a simple neuron that takes two inputs:</p>

            <pre><code>// Single Neuron 
let weights = [
	Math.random() * 2 - 1,
	Math.random() * 2 - 1
];
let bias    = Math.random() * 2 - 1;

// Evaluating a single neuron
function forward(inputs) {
  return inputs[0] * weights[0] + inputs[1] * weights[1] + bias;
}</code></pre>

            <p>
                When a neuron is initialized, it is filled with random values in the range of -1 to 1. Each weight / bias is a paramater. But why random initialization? If all paramaters started at zero, every neuron would learn the same thing. Randomness breaks symmetry and lets each neuron specialize.
            </p>

            <p>
                Having paramaters with random values produces random output. To generate more predicatble output, we need to find optimal values for each paramater. We can train the network to  adjust those paramaters automatically so the network produces better output. How Do We Train? With  <strong>Gradient Descent</strong>. Gradient Descent is guided trial and error, defined by this loop:
            </p>

            <ol>
                <li><strong>Predict</strong>: Run the neuron on an input -> get an output.</li>
                <li><strong>Measure error</strong>: Figure out how far off is the prediction from the target.</li>
                <li><strong>Compute gradients</strong>: Figure out which way to nudge each parameter (weights and bias) to reduce that error.</li>
                <li><strong>Update paramaters</strong>: Take a tiny step in that direction.</li>
            </ol>

            <p>Let's say we want our neuron to learn this mapping:</p>

            <pre><code>Input: [2, 1]  ->  Target Output: 5</code></pre>

            <p>To teach the neuron, we will run 100 training steps:</p>

            <pre><code>const trainingInput = [2, 1];
const targetOutput = 5;

for (let i = 0; i < 100; i++) {
  // 1) Predict
  const prediction = forward(trainingInput);

  // 2) Compute error
  const error = prediction - targetOutput;

  // 3) Compute gradients — "which way should each knob turn?"
  const weightGradients = [
    error * trainingInput[0],
    error * trainingInput[1]
  ];
  const biasGradient = error;

  // 4) Update parameters (move *against* the error)
  const learningRate = 0.01;
  weights[0] -= learningRate * weightGradients[0];
  weights[1] -= learningRate * weightGradients[1];
  bias       -= learningRate * biasGradient;

  // Log progress occasionally
  if (i % 20 === 0) {
    console.log(
      `Iter ${i}: pred=${prediction.toFixed(2)}, err=${error.toFixed(2)}, ` +
      `w=[${weights.map(w => w.toFixed(2)).join(", ")}], b=${bias.toFixed(2)}`
    );
  }
}</code></pre>

            <p>
                Run this, and you'll see the prediction inch closer to 5 with each step. The magic isn't in complexity, it's in repetition and feedback.
            </p>

            <h1>A trainable neural network</h1>

            <p>
                A single neuron is limited, it can only learn linear relationships. But stack them into layers, add non-linear activation functions, and suddenly you can learn anything!
            </p>

            <h2>Neuron</h2>

            <p>Let's formalize our neuron into a class. The neuron class has some number of weights and a bias. These are the neurons paramaters. We'll implement the forward pass as well:</p>

            <pre><code>class Neuron {
    weights = null;
    bias = null;

    constructor(numberOfInputs) {
        this.weights = new Array(numberOfInputs);
        for (let i = 0; i < numberOfInputs; i++) {
            this.weights[i] = Math.random() * 2 - 1; // [-1, 1]
        }
        this.bias = Math.random() * 2 - 1;
    }

    forward(inputs) {
        let sum = 0;
        for (let i = 0; i < inputs.length; i++) {
            sum += this.weights[i] * inputs[i];
        }
        return sum + this.bias;
    }
}</code></pre>

            <h2>Dense Layers</h2>

            <p>
                In a Neural Network, Neurons are organized into layers. The most common type is the dense layer (also called a fully connected layer). Every neuron in a dense layer is connected to all neurons of the previous layer. If we have a layer with <code>n</code> neurons, followed by a layer with <code>m</code> neurons, then the following hold true for the second layer:
            </p>
            <ol>
                <li>Each neuron holds <code>n</code> weights (one per input) + 1 bias.</li>
                <li>The layer as a whole learns <code>m</code> different ways to interpret the input.</li>
                <li>The output is an <code>m</code>-dimensional vector. It is a richer, transformed representation that will be passed to the next layer.</li>
            </ol>

            <p>
                This is where the network starts building hierarchical understanding: early layers might detect simple patterns, while deeper layers combine those into complex concepts.
            In code, a dense layer is implemented as an array of neurons:</p>

            <pre><code>class DenseLayer {
    neurons = null;

    constructor(numberOfInputs, numberOfOutputs) {
        this.neurons = new Array(numberOfOutputs);
        for (let i = 0; i < numberOfOutputs; i++) {
            this.neurons[i] = new Neuron(numberOfInputs);
        }
    }

    forward(inputs) {
        const result = new Array(inputs.length);
        for (let i = 0, size = inputs.length; i < size; ++i) {
            result[i] = this.neurons[i].forward(inputs);
        }
        return result;
    }
}</code></pre>

            <p>
                But wait, there's a catch... <strong>Dense layers are linear</strong>.
                No matter how many dense layers you stack, the whole network remains a fancy linear function.
            </p>

            <h2>Activation Layers</h2>

            <p>
                Activation functions add non-linearity to neural networks. Without them, your network is just a very expensive straight line. With them? It can learn curves.
            </p>

            <p>
                Instead of adding an activation function to each neuron, we're going to implement activation layers. The activation layer will apply it's activation function to each input value. Here's our activation layer:
            </p>

            <pre><code>class ActivationLayer {
    type = "relu";

    constructor(layerType = "relu") {
        this.type = layerType;
    }

    #reluActivation(x) {
        return Math.max(0, x);
    }

    #sigmoidActivation(x) {
        return 1 / (1 + Math.exp(-x));
    }

    #tanhActivation(x) {
        return Math.tanh(x);
    }

    forward(inputs) {
        const output = new Array(inputs.length);

        if (this.type === "relu") {
            for (let i = 0, size = inputs.length; i < size; ++i) {
                output[i] = this.#reluActivation(inputs[i]);
            }
        }
        else  if (this.type === "sigmoid") {
            for (let i = 0, size = inputs.length; i < size; ++i) {
                output[i] = this.#sigmoidActivation(inputs[i]);
            }
        }
        else  if (this.type === "tanh") {
            for (let i = 0, size = inputs.length; i < size; ++i) {
                output[i] = this.#tanhActivation(inputs[i]);
            }
        }
        else {
            return null;
        }

        return output;
    }
}</code></pre>

            <p>Why are we implementing these three activation functions specifically?</p>

            <ul>
                <li><strong>ReLU</strong> (max(0, x)) - is fast, simple, and works great in hidden layers.</li>
                <li><strong>tanh</strong> squashes values to [-1, 1] - smooth and centered, perfect for hidden units.</li>
                <li><strong>Sigmoid</strong> squashes to [0, 1] - ideal for binary outputs (like XOR's 0 or 1).</li>
            </ul>

            <p>By stacking dense layers followed by activation layers we can start to create simple neural networks.</p>

            <h2>Evaluating an XOR function</h2>

            <p>Let's build a tiny network that learns the XOR function. Given two inputs, this function returns true only if one input is true and the other one is false. Consider this truth table:</p>

            <table>
                <thead>
                    <tr>
                        <th>Input A</th>
                        <th>Input B</th>
                        <th>A XOR B</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                        <td>Both inputs false -> Output false</td>
                    </tr>
                    <tr>
                        <td>0</td>
                        <td>1</td>
                        <td>1</td>
                        <td>Different inputs -> Output true</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>0</td>
                        <td>1</td>
                        <td>Different inputs -> Output true</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>1</td>
                        <td>0</td>
                        <td>Both inputs true -> Output false</td>
                    </tr>
                </tbody>
            </table>

            <p>Solving this XOR function is impossible for a single neuron, but easy for a network.To solve this problem, we will design the following network:</p>
            <ul>
                <li>Input: 2 numbers (A, B)</li>
                <li>Hidden layer: 4 neurons -> tanh activation</li>
                <li>Output layer: 1 neuron -> sigmoid activation</li>
            </ul>

            <p>Let's translate that to codde:</p>

            <pre><code>function xor_ai(left, right) {
    // Create network
    const layer1 = new DenseLayer(2, 4);
    const activation1 = new ActivationLayer("tanh");
    const layer2 = new DenseLayer(4, 1);
    const activation2 = new ActivationLayer("sigmoid");

    // Load Weights (we will learn how to generate these later)
    layer1.neurons[0].weights[0] = -2.483405330352288;
    layer1.neurons[0].weights[1] = 3.746893395232311;
    layer1.neurons[0].bias = 0.8972583832821088;
    layer1.neurons[1].weights[0] = -3.653234475692758;
    layer1.neurons[1].weights[1] = 0.9955207401046027;
    layer1.neurons[1].bias = 0.5799612103320189;
    layer1.neurons[2].weights[0] = -1.986455463777911;
    layer1.neurons[2].weights[1] = -2.140729883658909;
    layer1.neurons[2].bias = 0.1773771186808191;
    layer1.neurons[3].weights[0] = -3.315691022651759;
    layer1.neurons[3].weights[1] = -3.478943831512278;
    layer1.neurons[3].bias = 1.0466264947557882;
    layer2.neurons[0].weights[0] = -5.891722647970969;
    layer2.neurons[0].weights[1] = 5.841764877092181;
    layer2.neurons[0].weights[2] = -2.167853628782214;
    layer2.neurons[0].weights[3] = -4.926738682524884;
    layer2.neurons[0].bias = -0.9654887789785622;

    // Run network
    let output = layer1.forward([left, right]);
    output = activation1.forward(output);
    output = layer2.forward(output);
    output = activation2.forward(output);

    console.log(`Input: [${left}, ${right}] -> Output: ${output[0].toFixed(3)}`);

    return output[0];
}</code></pre>

            <p>
                The <code>xor_ai</code> function provides pre-trained paramaters. But without training, if the paramaters where random, the output would also be random noise. To make this network smart, we need to train it.
            </p>

            <h2>The Training loop</h2>

            <p>
                We've built our network and it can make predictions. Terrible ones at first, but predictions nonetheless. How do we teach it? Trough  backpropagation, the algorithm that makes deep learning possible.
            </p>

            <blockquote>
                <p>Backpropagation is the chain rule from calculus, applied backwards through the network. When you have nested functions like <code>f(g(h(x)))</code>, the chain rule tells us: <code>df/dx = df/dg x dg/dh x dh/dx</code>. In neural networks: <code>x</code> is the input, each layer is a function transforming its input.</p>
            </blockquote>

            <p>
                Back propogation figures out how much each paramater is responsible for the error of the output. That gives us a direction and magnitude to adjust the paramater. Each weight only needs to know its local contribution to the error. No weight needs to understand the entire network-just its immediate neighborhood.
            </p>

            <h3>Measuring error</h3>

            <p>
                In our network's forward pass, data flows from input through layers to output. But how do we know if the output is any good? We need a way to measure error. Measuring the error of a network is done with a loss function. It's a function that tells us how wrong the network was.
            </p>

            <p>
                The most common loss function for regression tasks is Mean Squared Error (MSE). The function is simple, take the difference between what you predicted and what you wanted, square that difference (to make all errors positive), and average across all outputs.
            </p>

            <pre><code>class Loss {
    // Mean Squared Error (MSE) - most common for regression
    static meanSquaredError(predictions, targets) {
        // (1/n) * Σ(prediction - target)²
        let sum = 0;
        for (let i = 0, size = predictions.length; i < size; i++) {
            const diff = predictions[i] - targets[i];
            sum += diff * diff;
        }
        return sum / predictions.length;
    }

    // Derivative of MSE with respect to predictions
    static meanSquaredErrorDerivative(predictions, targets) {
        // ∂MSE/∂prediction[i] = (2/n) * (prediction[i] - target[i])
        const derivatives = new Array(predictions.length);
        for (let i = 0; i < predictions.length; i++) {
            derivatives[i] = 2 * (predictions[i] - targets[i]) / predictions.length;
        }
        return derivatives;
    }
}</code></pre>

            <p>Why do we square the error?</p>
            <ol>
                <li><strong>Makes all errors positive</strong>: We don't want negative and positive errors canceling out.</li>
                <li><strong>Punishes large errors more</strong>: A prediction that's off by 2 gets 4x the penalty of being off by 1. This pushes the network to avoid big mistakes.</li>
                <li><strong>It's differentiable</strong>: We need smooth gradients for backpropagation (more on this soon).</li>
            </ol>

            <p>We will need the derivitive soon. The derivitive of the loss function is our networks error.</p>

            <h3>Back propogation trough a neuron</h3>

            <p>
                Now let's implement backpropagation for a single neuron. Remember, a neuron computes <code>output = sum(weights x inputs) + bias</code>. During backpropagation, we need to reverse this process: given an error in the output, figure out how to adjust our parameters and what error to pass back to our inputs.
            </p>

            <p>In backpropagation, gradients flow backward while updates happen locally. Each neuron:</p>
            <ol>
                <li><strong>Receives</strong> gradient from the layer ahead (how wrong was my output)</li>
                <li><strong>Calculates</strong> parameter gradients (how to adjust weights and bias)</li>
                <li><strong>Calculates</strong> input gradients (error to pass backward, proportional to weights)</li>
                <li><strong>Updates</strong> its own parameters (weights and bias)</li>
                <li><strong>Returns</strong> input gradients to the previous layer</li>
            </ol>

            <pre><code>// ... class Neuron
    backward(inputs, /*1) gradient: */ neuronGradient, learningRate) {
        // 2) Calculate paramater gradients
        const parameterGradients = this.#calculateParameterGradients(inputs, neuronGradient);

        // 3) Calculate INPUT gradients (errors to pass backward)
        const inputGradients = this.#calculateInputGradients(neuronGradient);

        // 4) Update weights using parameter gradients
        this.#updateWeights(parameterGradients, learningRate);

        // 5) What the errors coming in from the last layer are. Pass them back.
        return inputGradients;
    }
// ...</code></pre>

            <h4>Calculating paramater gradients</h4>

            <p>
                We need to figure out how much each specific weight contributed to the error. We do this by looking at the input signal.Think of it as Sensitivity Analysis. The incoming <code>neuronGradient</code> tells us the direction and magnitude we want the output to move (e.g., "The output needs to be lower by 0.5"). <code>inputs[i]</code> acts as an amplifier for that request.
            </p>

            <ul>
                <li><strong>The Silent Input</strong> (Input = 0): If the input is 0, the math is Weight * 0. No matter how much you change the weight, the result is still 0. The output is insensitive to this weight. Therefore, the gradient is 0. We don't waste compute updating a weight that has no effect.</li>
                <li><strong>The Loud Input</strong> (Input = 5): If the input is 5, any tiny change to the weight is multiplied by 5 in the output. The output is highly sensitive to this weight. Therefore, we multiply the gradient by 5 so we prioritize updating the weights that give us the most "bang for our buck."</li>
            </ul>

            <p>
                We are essentially scaling our update effort by how "active" the connection was. In code, this relationship is linear. The "blame" assigned to a weight is simply the error coming from above (neuronGradient) scaled by the strength of the input signal (inputs[i]).
            </p>

            <pre><code>// ... class Neuron
    #calculateParameterGradients(inputs, neuronGradient) {
        const biasGradient = neuronGradient;

        // The gradient for weights depends on their input intensity
        const weightGradients = new Array(inputs.length);
        for (let i = 0; i < inputs.length; i++) {
            weightGradients[i] = neuronGradient * inputs[i];
        }

        return {
            bias: biasGradient,
            weights: weightGradients
        };
    }
// ...</code></pre>

            <h4>Calculating Input Gradients</h4>

            <p>
                In this step we propagate the gradient back to the previous layer. Conceptually, the neuron receives a single scalar gradient for its output (<code>neuronGradient</code>) and needs to distribute that signal across its inputs.
            </p>

            <pre><code>// ... class Neuron
    #calculateInputGradients(neuronGradient) {
        // These gradients are the errors we pass to the previous layer
        const inputGradients = new Array(this.weights.length);

        for (let i = 0; i < this.weights.length; i++) {
            inputGradients[i] = neuronGradient * this.weights[i];
        }

        return inputGradients;
    }
// ...</code></pre>

            <p>
                Remember, during the forward pass, each input is scaled by its weight before being added into the neuron's output: <code>sum += this.weights[i] * inputs[i];</code>
            </p>

            <p>
                During the backward pass, we're answering the question "If this neurons output was too high, how much is each input to blame?" When trying to distribute the error to any input, we have to consider that the neurons weight acts like a volume knob.
            </p>

            <ul>
                <li>If a weight is large (say, 5), then even a small input has a big effect on the output. So if the output is wrong, that input deserves a lot of "blame", a strong error signal to correct it.</li>
                <li>If a weight is tiny (say, 0.01), then the input barely moved the needle. Even if the output is way off, this input didn't contribute much, so it gets a tiny error signal.</li>
            </ul>

            <p>
                So to divide responsibility fairly, we multiply the incoming error (<code>neuronGradient</code>) by the weight that connected that input to the output: <code>inputGradients[i] = neuronGradient * this.weights[i];</code>
            </p>

            <p>
                <code>input[i]</code>, was scaled by <code>weight[i]</code> on the way forward. On the way back, <code>input[i]</code>s share of the error is scaled by that same amount.
            </p>

            <h4>Updating weights</h4>

            <p>
                Finally, it's time to adjust othe neurons parameters. This is where learning actually happens. We need to move <em>against</em> the gradient by subtracting. If the gradient says "increasing this weight increases error," we decrease the weight to decrease the error
            </p>

            <pre><code>// ... class Neuron
    #updateWeights(gradients, learningRate) {
        // Update each weight using its stored gradient
        for (let i = 0; i < this.weights.length; i++) {
            this.weights[i] -= learningRate * gradients.weights[i];
        }
        // Update bias using its stored gradient
        this.bias -= learningRate * gradients.bias;
    }
// ...</code></pre>

            <p>
                This code moves each parameter in the direction that reduces error. The <code>learningRate</code> controls step size, too large and training becomes unstable; too small and learning takes forever. Each weight and the bias are adjusted independently, using only local information: their own gradient and the global learning rate. No coordination or global knowledge is needed, this locality is what makes neural network training parallelizable and scalable.
            </p>

            <h3>Backwards trough a dense layer</h3>

            <p>
                As we've seen, back propogation trough a single neuron requires us to remember what the input values of that neuron where. To compute gradients, we need to cache the inputs of each layer. Let's add the caching to our <code>DenseLayer</code> object, which will be able to pass the same cached vector to each neuron:
            </p>

            <pre><code>class DenseLayer {
    neurons = null;
    cachedInputs = null; // NEW

    constructor(numberOfInputs, numberOfOutputs) {
        this.neurons = new Array(numberOfOutputs);
        for (let i = 0; i < numberOfOutputs; ++i) {
            this.neurons[i] = new Neuron(numberOfInputs);
        }
        this.cachedInputs = new Array(numberOfInputs); // NEW
    }

    forward(inputs) {
        // Cache inputs for the backwards pass
        for (let i = 0, size = inputs.length; i < size; ++i) {
            this.cachedInputs[i] = inputs[i];
        }

        const outputs = new Array(this.neurons.length);
        for (let i = 0, size = this.neurons.length;  i < size; ++i) {
            outputs[i] = this.neurons[i].forward(inputs);
        }
        return outputs;
    }
// ...</code></pre>

            <p>Both dense and activation layers need to do cache their inputs.</p>

            <pre><code>class ActivationLayer {
    kind = "relu";
    cachedInputs = null; // NEW

    forward(inputs) {
        // Cache inputs for the backwards pass
        if (this.cachedInputs == null || this.cachedInputs.length !== inputs.length) {
            this.cachedInputs = new Array(inputs.length);
        }
        for (let i = 0, size = inputs.length; i < size; ++i) {
            this.cachedInputs[i] = inputs[i];
        }
// ...</code></pre>

            <p>
                Each neuron knows how to update itself and pass error signals backward, but we need to coordinate this process across an entire layer. A dense layer contains many neurons, each receiving the same input vector but producing a unique output. During backpropagation, each neuron will receive its own error (from the layer ahead) and independently compute how much blame to assign to each input.
            </p>

            <pre><code>// ... class DenseLayer
    backward(outputGradients, learningRate) {
        // Start with zero error for each input
        const inputGradients = new Array(this.cachedInputs.length);
        for (let i = 0; i < inputGradients.length; i++) {
            inputGradients[i] = 0;  // Start at zero!
        }

        // Each neuron will ADD its blame to the inputs
        for (let neuronIdx = 0; neuronIdx < this.neurons.length; neuronIdx++) {
            const neuron = this.neurons[neuronIdx];

            // This neuron calculates: "how much did each input contribute to MY error?"
            const neuronsInputGradients = neuron.backward(
                this.cachedInputs,
                outputGradients[neuronIdx],  // This specific neuron's error
                learningRate
            );

            // ADD this neuron's blame to our running total
            for (let i = 0; i < neuronsInputGradients.length; i++) {
                inputGradients[i] += neuronsInputGradients[i];  // += not =
            }
        }

        return inputGradients;
    }
// ...</code></pre>

            <p>
                Every neuron in the layer contributes its own "blame" for each input, and we must sum those contributions together. Why? Because each input value fed into all neurons in the layer, so if multiple neurons were wrong, the input is responsible for all of those errors combined.
            </p>

            <p>
                The <code>backward</code> method in <code>DenseLayer</code> handles this by initializing an <code>inputGradients</code> vector to zero, then looping over every neuron. Each neuron runs its own <code>backward</code> pass using its specific output gradient (<code>outputGradients[neuronIdx]</code>) and returns how much error to assign to each input. These per-neuron input gradients are added into the shared inputGradients array. The result is a complete picture of how the entire layer's error depends on each input, exactly what the previous layer needs to continue backpropagation.
            </p>

            <h3>Backwards trough an activation layer</h3>

            <p>
                Activation layers don't have learnable parameters, but they still need to participate in backpropagation. During the forward pass, activation layers warped the signal: ReLU killed negative values, sigmoid squashed everything toward 0 or 1, tanh bent the curve into a smooth S. To reverse this warping during backpropagation, we need to know how sensitive the output was to the input at the exact point where we evaluated it.
            </p>

            <p>
                That's why we cached the original inputs during the forward pass: the derivative of an activation function depends on the input value, not the output. For example, ReLU's derivative is 1 when the input was positive (meaning small changes in input pass through unchanged) and 0 when the input was negative (meaning no change in output regardless of input). Without the original input, we couldn't compute the correct local slope.
            </p>

            <p>These are the derivitives of the three activation functions we have implemented:</p>

            <pre><code>// ... class ActivationLayer
    #reluDerivative(x) {
        return x > 0 ? 1 : 0;
    }

    #sigmoidDerivative(x) {
        const sig = this.#sigmoidActivation(x);
        return sig * (1 - sig);
    }

    #tanhDerivative(x) {
        const t = Math.tanh(x);
        return 1 - t * t;
    }
// ...</code></pre>

            <p>These derivitives have some interesting properties:</p>
            <ul>
                <li><strong>ReLU derivative</strong>: Binary. Either the gradient flows (x > 0) or it doesn't (x <= 0). This can cause "dead neurons" that stop learning.</li>
                <li><strong>Sigmoid derivative</strong>: Strongest in the middle, weak at extremes. This causes the "vanishing gradient" problem in deep networks.</li>
                <li><strong>Tanh derivative</strong>: Similar to sigmoid but generally better behaved.</li>
            </ul>

            <p>
                Now let's implement the backward pass of the Activation Layer. This pass propagates gradients through the activation function by scaling the incoming <code>outputGradients</code> with the derivative of the activation evaluated at the cached input.
            </p>

            <pre><code>// ... class ActivationLayer
    backward(outputGradients) {
        // outputGradients = error signal coming from the next layer
        // We need to figure out what the input gradient was

        const inputGradients = new Array(outputGradients.length);

        if (this.type === "relu") {
            for (let i = 0; i < outputGradients.length; i++) {
                inputGradients[i] = outputGradients[i] * this.#reluDerivative(this.cachedInputs[i]);
            }
        }
        else if (this.type === "sigmoid") {
            for (let i = 0; i < outputGradients.length; i++) {
                inputGradients[i] = outputGradients[i] * this.#sigmoidDerivative(this.cachedInputs[i]);
            }
        }
        else if (this.type === "tanh") {
            for (let i = 0; i < outputGradients.length; i++) {
                inputGradients[i] = outputGradients[i] * this.#tanhDerivative(this.cachedInputs[i]);
            }
        }

        return inputGradients;
    }
// ...</code></pre>

            <blockquote>
                <p>The activation layer doesn't have weights to update, it just modulates the gradient based on its derivative. Think of it as a gradient filter: ReLU is a gate (open or closed), while sigmoid and tanh are dimmers (gradual adjustment).</p>
            </blockquote>

            <h2>Training the XOR network</h2>

            <p>
                Let's put everything together into a trainable network! XOR is the "Hello World" of neural networks, simple enough to understand, complex enough to require hidden layers. A linear model can't solve XOR, but our network can.
            </p>

            <pre><code>// XOR truth table:
// 0, 0 -> 0
// 0, 1 -> 1
// 1, 0 -> 1
// 1, 1 -> 0

// Training data
const trainingData = [
    { input: [0, 0], target: [0] },
    { input: [0, 1], target: [1] },
    { input: [1, 0], target: [1] },
    { input: [1, 1], target: [0] }
];

// Build the network architecture
// 2 inputs -> 4 hidden neurons -> 1 output
const layer1 = new DenseLayer(2, 4);  // 2 inputs, 4 hidden neurons
const activation1 = new ActivationLayer("tanh");
const layer2 = new DenseLayer(4, 1);  // 4 inputs, 1 hidden neuron
const activation2 = new ActivationLayer("sigmoid");

// Training parameters
const learningRate = 0.5;
const epochs = 10000;

// Training loop
for (let epoch = 0; epoch < epochs; epoch++) {
    let totalLoss = 0;

    // Train on each example
    for (const data of trainingData) {
        // Forward pass
        let output = layer1.forward(data.input);
        output = activation1.forward(output);
        output = layer2.forward(output);
        output = activation2.forward(output);

        // Calculate loss
        const loss = Loss.meanSquaredError(output, data.target);
        totalLoss += loss;

        // Calculate initial gradient from loss
        let gradients = Loss.meanSquaredErrorDerivative(output, data.target);

        // Backward pass
        gradients = activation2.backward(gradients);
        gradients = layer2.backward(gradients, learningRate);
        gradients = activation1.backward(gradients);
        gradients = layer1.backward(gradients, learningRate);
    }

    // Print progress every 1000 epochs
    if (epoch % 1000 === 0) {
        console.log(`Epoch ${epoch}: Average Loss = ${totalLoss / trainingData.length}`);
    }
}

// Test the trained network
console.log("\n=== Testing Trained Network ===");
for (const data of trainingData) {
    // Forward pass only
    let output = layer1.forward(data.input);
    output = activation1.forward(output);
    output = layer2.forward(output);
    output = activation2.forward(output);

    console.log(`Input: [${data.input}] -> Output: ${output[0].toFixed(3)} (Target: ${data.target[0]})`);
}</code></pre>

            <p>
                Run this code and watch the loss drop! At first, the network is guessing randomly. But gradually, it discovers the XOR pattern. By epoch 10000, it should be able to answer all four cases.
            </p>

            <blockquote>
                <p>What's actually happening in those hidden neurons? They're learning features! One might activate for "exactly one input is 1", another for "both inputs are the same". The output layer then combines these features to compute XOR. It's feature engineering, automated.</p>
            </blockquote>

            <!--h1>What's Next?</h1>

            <p>We've built a neural network from scratch, but this is just the beginning. Real networks have:</p>

            <ul>
                <li><strong>Batch processing</strong>: Training on multiple examples at once for stability</li>
                <li><strong>Optimizers</strong>: Adam, RMSprop, smarter than vanilla gradient descent</li>
                <li><strong>Regularization</strong>: Dropout, L2 penalties-preventing overfitting</li>
                <li><strong>Convolutional layers</strong>: For images, detecting patterns regardless of position</li>
                <li><strong>Recurrent connections</strong>: For sequences, remembering previous inputs</li>
            </ul>

            <p>
                The core principles remain the same though. It's all forward passes, backward passes, and gradients flowing through differentiable functions. The complexity comes from scale.
            </p-->

            <h2>Resources</h2>

            <ul>
                <li><a href="https://gist.github.com/gszauer/587b0e0127a678895f68f38974bd9f16">Full source code listing</a></li>
                <li><a href="https://tchojnacki.dev/blog/neural-networks-in-plain-javascript"> Neural networks in plain JavaScript</a></li>
            </ul>
        </article>
    </main>

    <!-- Footer -->
     <footer>
    <div class="container" style="display:flex; justify-content:space-between; align-items:center; flex-wrap:wrap; gap:8px;">
      <div>&copy; 2025 <a href="https://gabormakesgames.com/">Gabor Szauer</a></div>
      <div style="display:flex; gap:1rem;">
        <a href="TLDR.md">TLDR.md</a>
        <a href="WebGL_Inference.md">WebGL_Inference.md</a>
        <a href="https://github.com/gszauer/Gab">GitHub</a>
        <a href="https://bsky.app/profile/gszauer.bsky.social">bsky</a>
      </div>
    </div>
  </footer>

    <script>
        // JavaScript syntax highlighting with proper tokenization
        function highlightJavaScript() {
            // Get all code blocks except ASCII diagrams
            const codeBlocks = document.querySelectorAll('pre:not(.ascii-diagram) code');

            codeBlocks.forEach(block => {
                // Add language class
                block.classList.add('language-javascript');

                // Get the text content
                const code = block.textContent;

                // Tokenize the code
                const tokens = tokenizeJavaScript(code);

                // Build HTML from tokens
                let html = '';
                for (const token of tokens) {
                    if (token.type === 'plain') {
                        html += escapeHtml(token.value);
                    } else {
                        html += `<span class="${token.type}">${escapeHtml(token.value)}</span>`;
                    }
                }

                // Set the highlighted HTML
                block.innerHTML = html;
            });
        }

        function tokenizeJavaScript(code) {
            const tokens = [];
            let i = 0;

            while (i < code.length) {
                let matched = false;

                // Skip whitespace but preserve it
                if (/\s/.test(code[i])) {
                    let start = i;
                    while (i < code.length && /\s/.test(code[i])) i++;
                    tokens.push({ type: 'plain', value: code.substring(start, i) });
                    continue;
                }

                // Comments
                if (code[i] === '/' && i + 1 < code.length) {
                    if (code[i + 1] === '/') {
                        // Single line comment
                        let start = i;
                        i += 2;
                        while (i < code.length && code[i] !== '\n') i++;
                        tokens.push({ type: 'comment', value: code.substring(start, i) });
                        continue;
                    } else if (code[i + 1] === '*') {
                        // Multi-line comment
                        let start = i;
                        i += 2;
                        while (i + 1 < code.length && !(code[i] === '*' && code[i + 1] === '/')) i++;
                        if (i + 1 < code.length) i += 2;
                        tokens.push({ type: 'comment', value: code.substring(start, i) });
                        continue;
                    }
                }

                // Strings
                if (code[i] === '"' || code[i] === "'" || code[i] === '`') {
                    const quote = code[i];
                    let start = i;
                    i++;
                    while (i < code.length && code[i] !== quote) {
                        if (code[i] === '\\' && i + 1 < code.length) {
                            i += 2; // Skip escaped character
                        } else {
                            i++;
                        }
                    }
                    if (i < code.length) i++; // Include closing quote
                    tokens.push({ type: 'string', value: code.substring(start, i) });
                    continue;
                }

                // Numbers
                if (/\d/.test(code[i]) || (code[i] === '.' && i + 1 < code.length && /\d/.test(code[i + 1]))) {
                    let start = i;
                    while (i < code.length && /[\d.]/.test(code[i])) i++;
                    tokens.push({ type: 'number', value: code.substring(start, i) });
                    continue;
                }

                // Identifiers and keywords
                if (/[a-zA-Z_$]/.test(code[i])) {
                    let start = i;
                    while (i < code.length && /[a-zA-Z0-9_$]/.test(code[i])) i++;
                    const word = code.substring(start, i);

                    // Check if it's a keyword
                    const keywords = ['const', 'let', 'var', 'function', 'class', 'if', 'else', 'for', 'while', 'do',
                        'switch', 'case', 'break', 'continue', 'return', 'new', 'this', 'typeof', 'instanceof',
                        'try', 'catch', 'finally', 'throw', 'extends', 'implements', 'static', 'import', 'export',
                        'from', 'as', 'default', 'async', 'await', 'yield', 'of', 'in', 'debugger', 'with', 'delete', 'void'];

                    const booleans = ['true', 'false'];
                    const nullish = ['null', 'undefined'];

                    let type = 'plain';
                    if (keywords.includes(word)) {
                        type = 'keyword';
                    } else if (booleans.includes(word)) {
                        type = 'boolean';
                    } else if (word === 'null') {
                        type = 'null';
                    } else if (word === 'undefined') {
                        type = 'undefined';
                    } else {
                        // Check if it's a function call (followed by parenthesis)
                        let j = i;
                        while (j < code.length && /\s/.test(code[j])) j++;
                        if (j < code.length && code[j] === '(') {
                            type = 'function';
                        } else if (word[0] === word[0].toUpperCase() && word[0] !== word[0].toLowerCase()) {
                            // Check if previous token was 'new', 'class', or 'extends' keyword
                            let prevToken = tokens[tokens.length - 1];
                            while (prevToken && prevToken.type === 'plain' && prevToken.value.trim() === '') {
                                prevToken = tokens[tokens.length - 2];
                            }
                            if (prevToken && prevToken.type === 'keyword' &&
                                ['new', 'class', 'extends'].includes(prevToken.value)) {
                                type = 'class-name';
                            }
                        }
                    }

                    tokens.push({ type, value: word });
                    continue;
                }

                // Operators and punctuation
                const operators = [
                    '===', '!==', '==', '!=', '<=', '>=', '<<', '>>', '>>>', '+=', '-=', '*=', '/=', '%=',
                    '++', '--', '&&', '||', '<', '>', '+', '-', '*', '/', '%', '=', '!', '?', ':', '&', '|', '^', '~'
                ];

                // Try to match multi-character operators first
                let operatorMatched = false;
                for (let len = 3; len >= 1; len--) {
                    const potential = code.substring(i, i + len);
                    if (operators.includes(potential)) {
                        tokens.push({ type: 'operator', value: potential });
                        i += len;
                        operatorMatched = true;
                        break;
                    }
                }

                if (operatorMatched) continue;

                // Punctuation
                if (/[{}()\[\];,.]/.test(code[i])) {
                    tokens.push({ type: 'punctuation', value: code[i] });
                    i++;
                    continue;
                }

                // Default: treat as plain text
                tokens.push({ type: 'plain', value: code[i] });
                i++;
            }

            return tokens;
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        // Run highlighting when page loads
        document.addEventListener('DOMContentLoaded', () => {
            highlightJavaScript();
            drawNeuronDiagram();
        });

        // Draw the neuron diagram
        function drawNeuronDiagram() {
            const canvas = document.getElementById('neuronDiagram');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;

            // Clear canvas
            ctx.clearRect(0, 0, width, height);

            // Set up styles (more muted pinks)
            const pink = '#b8356f';  // More muted, closer to outline
            const pink2 = '#c86a9a'; // More muted secondary
            const text = '#e9e9f2';
            const muted = '#a5a7bf';
            const panel = '#12121c';
            const line = '#2a2a3a';

            // Neuron box dimensions (adjusted for smaller canvas)
            const neuronWidth = 180;
            const neuronHeight = 120;
            const neuronX = (width - neuronWidth) / 2;
            const neuronY = (height - neuronHeight) / 2;

            // Input positions (adjusted spacing)
            const input0Y = neuronY + 35;
            const input1Y = neuronY + 85;
            const inputX = 80;
            const inputCircleRadius = 8;

            // Output position
            const outputX = width - 80;
            const outputY = neuronY + 60;

            // Draw neuron box (no glow)
            ctx.fillStyle = panel;
            ctx.fillRect(neuronX, neuronY, neuronWidth, neuronHeight);

            // Draw neuron box border
            ctx.strokeStyle = pink2;
            ctx.lineWidth = 2;
            ctx.strokeRect(neuronX, neuronY, neuronWidth, neuronHeight);

            // Draw dividing lines in neuron
            ctx.strokeStyle = line;
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(neuronX, neuronY + 30);
            ctx.lineTo(neuronX + neuronWidth, neuronY + 30);
            ctx.stroke();

            ctx.beginPath();
            ctx.moveTo(neuronX + neuronWidth/2, neuronY + 30);
            ctx.lineTo(neuronX + neuronWidth/2, neuronY + neuronHeight);
            ctx.stroke();

            // Draw text labels
            ctx.font = 'bold 14px ui-monospace, monospace';
            ctx.fillStyle = text;
            ctx.textAlign = 'center';
            ctx.fillText('Neuron', neuronX + neuronWidth/2, neuronY + 20);

            ctx.font = '12px ui-monospace, monospace';
            ctx.fillStyle = muted;
            ctx.textAlign = 'center';
            ctx.fillText('Weight0', neuronX + neuronWidth/4, neuronY + 55);
            ctx.fillText('Weight1', neuronX + neuronWidth/4, neuronY + 105);
            ctx.fillText('Bias', neuronX + 3*neuronWidth/4, neuronY + 80);

            // Draw input labels
            ctx.font = '12px ui-monospace, monospace';
            ctx.fillStyle = text;
            ctx.textAlign = 'right';
            ctx.fillText('Input0', inputX - 20, input0Y + 4);
            ctx.fillText('Input1', inputX - 20, input1Y + 4);

            // Draw output label
            ctx.textAlign = 'left';
            ctx.fillText('Output', outputX + 20, outputY + 4);

            // Draw input nodes
            ctx.fillStyle = pink2;
            ctx.strokeStyle = pink;
            ctx.lineWidth = 2;

            // Input0 node
            ctx.beginPath();
            ctx.arc(inputX, input0Y, inputCircleRadius, 0, Math.PI * 2);
            ctx.fill();
            ctx.stroke();

            // Input1 node
            ctx.beginPath();
            ctx.arc(inputX, input1Y, inputCircleRadius, 0, Math.PI * 2);
            ctx.fill();
            ctx.stroke();

            // Output node
            ctx.beginPath();
            ctx.arc(outputX, outputY, inputCircleRadius, 0, Math.PI * 2);
            ctx.fill();
            ctx.stroke();

            // Draw connections with arrows
            ctx.strokeStyle = pink2;
            ctx.lineWidth = 2;

            // Helper function to draw arrow
            function drawArrow(fromX, fromY, toX, toY) {
                ctx.beginPath();
                ctx.moveTo(fromX, fromY);
                ctx.lineTo(toX, toY);
                ctx.stroke();

                // Arrowhead
                const angle = Math.atan2(toY - fromY, toX - fromX);
                const arrowLength = 10;
                const arrowAngle = Math.PI / 6;

                ctx.beginPath();
                ctx.moveTo(toX, toY);
                ctx.lineTo(
                    toX - arrowLength * Math.cos(angle - arrowAngle),
                    toY - arrowLength * Math.sin(angle - arrowAngle)
                );
                ctx.moveTo(toX, toY);
                ctx.lineTo(
                    toX - arrowLength * Math.cos(angle + arrowAngle),
                    toY - arrowLength * Math.sin(angle + arrowAngle)
                );
                ctx.stroke();
            }

            // Input0 to neuron
            drawArrow(inputX + inputCircleRadius, input0Y, neuronX - 5, input0Y);

            // Input1 to neuron
            drawArrow(inputX + inputCircleRadius, input1Y, neuronX - 5, input1Y);

            // Neuron to output
            drawArrow(neuronX + neuronWidth + 5, outputY, outputX - inputCircleRadius, outputY);
        }
    </script>
</body>
</html>
