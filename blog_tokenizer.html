<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Part 2: Tokenization</title>
    <meta name="theme-color" content="#0A0A0F"/>
    <meta name="description" content="Language models don't see text the way we do. They see sequences of tokens, numerical representations of text or characters. This tutorial walks you through building a complete BPE tokenizer using plain JavaScript." />
    <style>
        :root{
            --bg:#0a0a0f;
            --bg-2:#0e0e16;
            --panel:#12121c;
            --panel-2:#171726;
            --text:#e9e9f2;
            --muted:#a5a7bf;
            --line:#1e1e2e;
            --pink:#ff2e88;
            --pink-2:#ff86c3;
            --glow: 0 0 .5rem var(--pink), 0 0 1.25rem color-mix(in oklab, var(--pink) 70%, white 0%), 0 0 2.5rem color-mix(in oklab, var(--pink) 35%, black 65%);
            --radius:16px;
            --pad: clamp(14px, 1.6vw, 22px);
            --font: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", "Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";
            --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        *{box-sizing:border-box}
        html,body{min-height:100%}
        body{
            margin:0;
            font-family:var(--font);
            color:var(--text);
            background:
                radial-gradient(1200px 520px at 50% -160px, rgba(255,46,136,.12), transparent 75%),
                linear-gradient(180deg, #090914 0%, #050509 45%, #030306 75%, #020203 100%);
            background-color:#020203;
            background-repeat:no-repeat, no-repeat;
            background-size:130% 70%, 100% 100%;
            background-position:center top, center top;
            background-attachment: fixed;
            line-height:1.6;
            overflow-x:hidden;
            -webkit-font-smoothing:antialiased;
            -moz-osx-font-smoothing:grayscale;
        }

        /* Subtle scanlines for retro vibe */
        body::before{
            content:"";
            position:fixed; inset:0;
            pointer-events:none;
            background-image: linear-gradient(rgba(255,255,255,.03),rgba(255,255,255,0) 2px);
            background-size:100% 3px; mix-blend-mode:overlay; opacity:.25;
        }
        @media (prefers-reduced-motion: reduce){
            body::before{display:none}
            *{animation: none !important; transition: none !important}
        }

        .container{width:min(1000px, 92vw); margin-inline:auto}

        /* Header */
        header{
            background: linear-gradient(180deg, #131321, #11111c);
            border-bottom: 1px solid var(--line);
            padding: 1.5rem 0;
            position: sticky;
            top: 0;
            z-index: 10;
            backdrop-filter: blur(10px);
        }

        nav{
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 1rem;
        }

        .logo{
            font-weight: 900;
            font-size: 1.25rem;
            color: var(--text);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .logo .pink{
            color: var(--pink);
            text-shadow: var(--glow);
        }

        .back-btn{
            appearance:none;
            border:1px solid color-mix(in oklab, var(--pink) 35%, #ffffff00 65%);
            color:var(--text);
            background: linear-gradient(180deg, var(--panel) 0%, var(--panel-2) 100%);
            padding:.45rem .85rem;
            border-radius: 999px;
            cursor:pointer;
            font-weight:600;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 0.35rem;
            font-size: 0.9rem;
            transition: transform .12s ease, box-shadow .15s ease;
        }

        .back-btn:hover{
            box-shadow: var(--glow);
            transform: translateY(-1px);
        }

        /* Article Hero */
        .article-hero{
            padding: 3rem 0 2rem;
            position: relative;
        }

        .grid-bg{
            position:absolute; inset:0; pointer-events:none; z-index:-1; opacity:.25;
            background:
                radial-gradient(circle at 50% -60px, color-mix(in oklab, var(--pink) 30%, transparent), transparent 35%),
                repeating-linear-gradient(0deg, #ffffff10 0 1px, transparent 1px 36px),
                repeating-linear-gradient(90deg, #ffffff10 0 1px, transparent 1px 36px);
            mask: radial-gradient(1200px 600px at 50% 0, black 40%, transparent 80%);
        }

        h1{
            font-size: clamp(32px, 5vw, 48px);
            line-height: 1.1;
            font-weight: 900;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--text) 0%, var(--pink) 100%);
            -webkit-background-clip: text;
            background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .article-meta{
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            color: var(--muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }

        .article-meta span{
            display: flex;
            align-items: center;
            gap: 0.35rem;
        }

        .article-blurb{
            font-size: 1.125rem;
            line-height: 1.6;
            color: #d9d9ec;
            max-width: 800px;
        }

        /* Article Content */
        article{
            background: linear-gradient(180deg, #131321, #11111c);
            border: 1px solid #222238;
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 2rem 0 3rem;
            box-shadow: 0 18px 50px rgba(0,0,0,.45), 0 0 60px -10px rgba(255,46,136,.15);
        }

        article h1{
            color: var(--text);
            margin: 2.5rem 0 1rem 0;
            font-size: 2.25rem;
            font-weight: 900;
            position: relative;
            padding-bottom: 0.75rem;
            background: none;
            -webkit-text-fill-color: var(--text);
        }

        article h1::after{
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 80px;
            height: 3px;
            background: linear-gradient(90deg, var(--pink), transparent);
        }

        article h1:first-child{
            margin-top: 0;
        }

        article h2{
            color: var(--text);
            margin: 2.5rem 0 1rem 0;
            font-size: 1.875rem;
            font-weight: 800;
            position: relative;
            padding-bottom: 0.75rem;
        }

        article h2::after{
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 2px;
            background: linear-gradient(90deg, var(--pink), transparent);
        }

        article h2:first-child{
            margin-top: 0;
        }

        article h3{
            color: var(--text);
            margin: 2rem 0 1rem 0;
            font-size: 1.375rem;
            font-weight: 700;
        }

        article h4{
            color: var(--text);
            margin: 1.5rem 0 0.75rem 0;
            font-size: 1.125rem;
            font-weight: 600;
        }

        article p{
            margin-bottom: 1.25rem;
            color: #d9d9ec;
            line-height: 1.7;
        }

        article ul, article ol{
            margin: 1.25rem 0;
            padding-left: 1.75rem;
            color: #d9d9ec;
        }

        article li{
            margin-bottom: 0.5rem;
            line-height: 1.7;
        }

        article strong{
            color: var(--text);
            font-weight: 600;
        }

        /* Links within article content */
        article a{
            color: var(--pink-2);
            text-decoration: none;
            border-bottom: 1px solid rgba(255, 134, 195, 0.3);
            transition: all 0.2s ease;
            font-weight: 500;
        }

        article a:hover{
            color: var(--pink);
            border-bottom-color: var(--pink);
            text-shadow: 0 0 8px rgba(255, 46, 136, 0.4);
        }

        article a:visited{
            color: color-mix(in oklab, var(--pink-2) 80%, var(--muted) 20%);
        }

        /* Code blocks */
        pre{
            background: #0f0f1b;
            border: 1px solid #2a2a3a;
            color: #e2e8f0;
            padding: 1.25rem;
            border-radius: 12px;
            margin: 1.5rem 0;
            overflow-x: auto;
            font-family: var(--mono);
            font-size: 0.875rem;
            line-height: 1.6;
            box-shadow: inset 0 2px 8px rgba(0,0,0,.3);
        }

        code{
            font-family: var(--mono);
            font-size: 0.9em;
            background: rgba(255, 46, 136, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            color: var(--pink-2);
        }

        pre code{
            background: none;
            padding: 0;
            color: #e2e8f0;
        }

        /* JavaScript Syntax Highlighting */
        .language-javascript .keyword { color: #ff79c6; font-weight: 600; }
        .language-javascript .function { color: #8be9fd; }
        .language-javascript .string { color: #f1fa8c; }
        .language-javascript .number { color: #bd93f9; }
        .language-javascript .comment { color: #6272a4; font-style: italic; }
        .language-javascript .operator { color: #ff79c6; }
        .language-javascript .class-name { color: #50fa7b; }
        .language-javascript .constant { color: #ff79c6; }
        .language-javascript .punctuation { color: #a5a7bf; }
        .language-javascript .boolean { color: #bd93f9; font-weight: 600; }
        .language-javascript .null { color: #bd93f9; font-weight: 600; }
        .language-javascript .undefined { color: #bd93f9; font-weight: 600; }

        /* Tables */
        table{
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: var(--panel);
            border: 1px solid #2a2a3a;
            border-radius: 8px;
            overflow: hidden;
        }

        th{
            background: linear-gradient(180deg, color-mix(in oklab, var(--pink) 15%, var(--panel) 85%), var(--panel-2));
            color: var(--text);
            padding: 0.75rem;
            text-align: left;
            font-weight: 600;
            border-bottom: 1px solid color-mix(in oklab, var(--pink) 30%, #2a2a3a 70%);
        }

        td{
            padding: 0.75rem;
            border-bottom: 1px solid #2a2a3a;
            color: #d9d9ec;
        }

        tr:last-child td{
            border-bottom: none;
        }

        tr:hover{
            background: rgba(255, 46, 136, 0.05);
        }

        /* Blockquotes */
        blockquote{
            border-left: 3px solid var(--pink);
            background: #0a0a14;
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            border-radius: 4px;
            font-style: italic;
            color: #d9d9ec;
        }

        blockquote p{
            margin-bottom: 0.5rem;
            margin-top: 0;
        }

        blockquote p:last-child{
            margin-bottom: 0;
        }

        /* Navigation */
        .article-nav{
            display: flex;
            justify-content: space-between;
            gap: 1rem;
            margin: 3rem 0;
            padding-top: 2rem;
            border-top: 1px solid var(--line);
        }

        .nav-link{
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--pink-2);
            text-decoration: none;
            font-weight: 600;
            transition: transform 0.2s;
        }

        .nav-link:hover{
            color: var(--pink);
            transform: translateX(3px);
        }

        .nav-link.prev:hover{
            transform: translateX(-3px);
        }

        /* Footer */
        footer{
            border-top: 1px solid var(--line);
            background: #0a0a12;
            color: var(--muted);
            padding: 1rem 0 1.5rem;
            font-size: 0.875rem;
            margin-top: 4rem;
        }

        footer a{
            color: var(--pink-2);
            text-decoration: none;
            border-bottom: 1px solid rgba(255, 134, 195, 0.3);
            transition: all 0.2s ease;
        }

        footer a:hover{
            color: var(--pink);
            border-bottom-color: var(--pink);
            text-shadow: 0 0 8px rgba(255, 46, 136, 0.4);
        }

        /* Responsive */
        @media (max-width: 768px){
            article{
                padding: 1.5rem;
            }

            h1{
                font-size: 2rem;
            }

            .article-meta{
                font-size: 0.8rem;
                gap: 1rem;
            }

            pre{
                padding: 1rem;
                font-size: 0.75rem;
            }
        }
    </style>
</head>
<body>
    <!-- Header/Navigation -->
    <header>
        <nav class="container">
            <a href="index.html" class="logo">
                The Gift of <span class="pink">Gab</span>
            </a>
            <a href="index.html" class="back-btn">
                Back to Home
            </a>
        </nav>
    </header>

    <!-- Main Article Content -->
    <main class="container">
        <article>
            <h1>Part 2: Tokenization</h1>

            <p>
                Language models don't see text the way we do. They see sequences of tokens, numerical representations of text or characters. This tutorial walks you through building a complete BPE tokenizer using plain JavaScript. We'll implement each component step by step.
            </p>

            <h1>Understanding Byte Pair Encoding</h1>

            <p>
                At its core, BPE is a compression algorithm that merges frequent byte pairs into single tokens. It starts with individual bytes as the base vocabulary, then iteratively merges the most frequent adjacent pairs until reaching a target vocabulary size. This simple process creates an effective tokenization scheme that balances vocabulary size with representation efficiency.
            </p>

            <pre><code>Initial:    [72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]  // "Hello World" as bytes
After BPE:  [256, 257, 258]  // Merged frequent pairs into new tokens</code></pre>

            <blockquote>
                <p>BPE learns which character sequences appear frequently together and creates shortcuts for them. The word "the" appears so often in English that it becomes a single token rather than three separate byte tokens.</p>
            </blockquote>

            <h1>The Tokenizer Architecture</h1>

            <p>Our tokenizer needs to handle several responsibilities:</p>
            <ol>
                <li><strong>Convert between text and bytes</strong> - All text ultimately becomes bytes</li>
                <li><strong>Learn merges from training data</strong> - Find patterns in the training corpus</li>
                <li><strong>Encode text to tokens</strong> - Apply learned merges to new text</li>
                <li><strong>Decode tokens back to text</strong> - Reverse the encoding process</li>
                <li><strong>Reserve special tokens</strong> - Pre-create merge chains for special sequences</li>
            </ol>

            <p>Let's translate this system into code:</p>

            <pre><code>class Tokenizer {
    merges = new Map();      // Maps byte pairs to new token IDs (key is a string, value is bytes array)
    vocabulary = new Map();  // Maps token IDs to their byte sequences (key is an integer, value is bytes array)
    nextTokenId = 256;       // Start after single-byte tokens (0-255)

    constructor() {
        // Initialize vocabulary with single-byte tokens
        for (let i = 0; i < 256; i++) {
            this.vocabulary.set(i, [i]);
        }
    }
}</code></pre>

            <p>
                The tokenizer starts with 256 base tokens - one for each possible byte value. Why bytes instead of characters? <strong>Unicode safety</strong>. By working at the byte level, we can handle any text encoding without worrying about character boundaries or encoding issues.
            </p>

            <h2>Converting Between Text and Bytes</h2>

            <p>
                Before we can tokenize, we need robust conversion between strings and bytes. JavaScript's TextEncoder/TextDecoder handle UTF-8 encoding:
            </p>

            <pre><code>#stringToBytes(text) {
    const encoder = new TextEncoder();
    const uint8Array = encoder.encode(text);
    const bytes = [];
    for (let i = 0; i < uint8Array.length; i++) {
        bytes.push(uint8Array[i]);
    }
    return bytes;
}

#bytesToString(bytes) {
    return new TextDecoder().decode(new Uint8Array(bytes));
}</code></pre>

            <p>
                The <code>#stringToBytes</code> method converts a string into an array of byte values (0-255). We explicitly create a regular JavaScript array rather than returning the Uint8Array directly because our token arrays need to hold values larger than 255 - after merging, we'll have token IDs like 256, 257, and beyond. A Uint8Array is limited to values 0-255, but a regular array can hold any JavaScript number.
            </p>

            <p>
                The <code>#bytesToString</code> reverses the process, converting byte arrays back to text. We wrap the input in <code>new Uint8Array()</code> to handle both regular arrays and typed arrays.
            </p>

            <h2>The Core Helper Functions</h2>

            <p>
                The heart of our BPE implementation lies in two helper functions that handle merge creation and application. These functions do the heavy lifting and are used throughout the tokenizer:
            </p>

            <h3>Creating Merges</h3>

            <p>This function creates a new token that represents the merging of two existing tokens:</p>

            <pre><code>#makeMerge(token1, token2) {
    // Check if this merge already exists
    const mergeKey = `${token1},${token2}`;
    const existingMerge = this.merges.get(mergeKey);
    if (existingMerge !== undefined) {
        return existingMerge;
    }

    // Create new token ID
    const newTokenId = this.nextTokenId++;

    // Store the merge rule
    this.merges.set(mergeKey, newTokenId);

    // Store what this new token represents
    const token1Bytes = this.vocabulary.get(token1);
    const token2Bytes = this.vocabulary.get(token2);
    const newTokenBytes = [...token1Bytes, ...token2Bytes];
    this.vocabulary.set(newTokenId, newTokenBytes);

    return newTokenId;
}</code></pre>

            <ol>
                <li><strong>Checks for existing merges</strong> - If we've already merged these tokens, reuse the existing ID. This is crucial for special tokens that might create overlapping merges.</li>
                <li><strong>Creates a merge rule</strong> - The key <code>"token1,token2"</code> maps to the new token ID, telling the encoder "when you see these two tokens together, replace them with this new token."</li>
                <li><strong>Updates the vocabulary</strong> - We store what bytes the new token represents by concatenating the byte sequences of its components.</li>
                <li><strong>Manages token IDs</strong> - Auto-increments <code>nextTokenId</code> to ensure unique IDs.</li>
                <li><strong>Preserves priority order</strong> - Because JavaScript Maps maintain insertion order, merges added first (more frequent patterns) will be applied first during encoding.</li>
            </ol>

            <blockquote>
                <p>The merge key format (<code>"token1,token2"</code>) uses string concatenation for simplicity. In production, you might use a more efficient structure. Maps preserve insertion order is critical, it means our merge priority is implicitly stored without needing a separate priority field.</p>
            </blockquote>

            <h3>Applying Merges</h3>

            <p>This function scans through a token sequence and replaces all occurrences of a specific pair:</p>

            <pre><code>#applyMerge(tokens, token1, token2, mergedTokenId) {
    const result = [];
    let i = 0;

    while (i < tokens.length) {
        if (i < tokens.length - 1 &&
            tokens[i] === token1 &&
            tokens[i + 1] === token2) {
            // Found the pair - replace with merged token
            result.push(mergedTokenId);
            i += 2;
        } else {
            // No merge here, keep original token
            result.push(tokens[i]);
            i += 1;
        }
    }

    return result;
}</code></pre>

            <p>This function implements a single-pass merge operation. It:</p>
            <ol>
                <li><strong>Scans left to right</strong> - Processes tokens in order, ensuring deterministic merging</li>
                <li><strong>Applies greedily</strong> - When it finds a matching pair, it immediately merges and skips both tokens</li>
                <li><strong>Preserves non-matching tokens</strong> - Tokens that don't match the merge pattern pass through unchanged</li>
                <li><strong>Returns a new array</strong> - Never mutates the input, following functional programming principles</li>
            </ol>

            <p>
                The greedy approach is important: if we have tokens <code>[1, 2, 2]</code> and we're merging <code>(2, 2)</code>, we'll get <code>[1, merged]</code>, not <code>[merged, 2]</code>. The first match wins.
            </p>

            <h2>Finding Frequent Pairs</h2>

            <p>The training process needs to identify which byte pairs appear most frequently:</p>

            <pre><code>#findMostFrequentPair(tokensList) {
    const pairCounts = new Map();

    // Count all adjacent pairs
    for (let i = 0; i < tokensList.length - 1; i++) {
        const pair = `${tokensList[i]},${tokensList[i + 1]}`;
        const currentCount = pairCounts.get(pair) || 0;
        pairCounts.set(pair, currentCount + 1);
    }

    // Find the most frequent pair
    let maxCount = 0;
    let mostFrequentPair = null;

    for (const [pair, count] of pairCounts) {
        if (count > maxCount) {
            maxCount = count;
            mostFrequentPair = pair;
        }
    }

    // Return as array of token IDs, or null if no pairs exist
    if (mostFrequentPair && maxCount > 1) {
        const tokens = mostFrequentPair.split(',');
        return [parseInt(tokens[0]), parseInt(tokens[1])];
    }

    return null;
}</code></pre>

            <p>
                This method scans through the token list, counting every adjacent pair. It uses the same string key format (<code>"token1,token2"</code>) as our merge rules for consistency. The method returns the most frequent pair as an array of two token IDs, or <code>null</code> if no pair appears more than once.
            </p>

            <blockquote>
                <p>Why require <code>maxCount > 1</code>? A pair that appears only once isn't worth merging - it won't save space or improve efficiency.</p>
            </blockquote>

            <h2>Training the Tokenizer</h2>

            <p>
                Training builds the merge rules by iteratively finding and merging the most frequent pairs. With our helper functions, the training logic becomes beautifully clear:
            </p>

            <pre><code>train(trainingText, numMerges) {
    let tokens = this.#stringToBytes(trainingText);

    console.log(`Starting training with ${tokens.length} bytes`);

    for (let mergeNum = 0; mergeNum < numMerges; mergeNum++) {
        // Find the most frequent pair
        const pair = this.#findMostFrequentPair(tokens);

        if (!pair) {
            console.log(`Training stopped early at merge ${mergeNum} - no more frequent pairs`);
            break;
        }

        const [token1, token2] = pair;

        // Create the merge (or get existing one if it already exists)
        const newTokenId = this.#makeMerge(token1, token2);

        // Apply the merge to all occurrences in the training data
        tokens = this.#applyMerge(tokens, token1, token2, newTokenId);

        // Log progress periodically
        if (mergeNum % 100 === 0 || mergeNum < 10) {
            const originalLength = this.#stringToBytes(trainingText).length;
            const compression = ((1 - tokens.length / originalLength) * 100).toFixed(1);
            console.log(`Merge ${mergeNum}: [${token1}, ${token2}] -> ${newTokenId}, compression: ${compression}%`);
        }
    }

    console.log(`Training complete. Vocabulary size: ${this.vocabulary.size}`);
    return this.merges.size;
}</code></pre>

            <p>The training algorithm follows these 4 steps:</p>
            <ol>
                <li><strong>Find the most common pair</strong> using <code>#findMostFrequentPair</code></li>
                <li><strong>Create a merge rule</strong> using <code>#makeMerge</code></li>
                <li><strong>Apply it everywhere</strong> using <code>#applyMerge</code></li>
                <li><strong>Repeat</strong> until we've created enough merges</li>
            </ol>

            <h2>Encoding Text to Tokens</h2>

            <p>
                Encoding applies learned merge rules to new text. Merges should be applied in the order they were learned during training:
            </p>

            <pre><code>encode(textToEncode) {
    let tokens = this.#stringToBytes(textToEncode);

    // Apply merges in the order they were learned
    // JavaScript Maps maintain insertion order, so iterating gives us merges
    // in the same order they were added during training
    for (const [mergeKey, mergedToken] of this.merges) {
        // Parse the merge key to get the two tokens to merge
        const [token1, token2] = mergeKey.split(',').map(Number);

        // Apply this merge wherever it appears in the sequence
        tokens = this.#applyMerge(tokens, token1, token2, mergedToken);
    }

    return tokens;
}</code></pre>

            <p>The encoding process:</p>
            <ol>
                <li><strong>Converts text to bytes</strong> - Starts with the raw byte representation</li>
                <li><strong>Applies merges in learned order</strong> - Iterates through the Map of merges, which preserves insertion order</li>
                <li><strong>Each merge is applied globally</strong> - When we apply a merge, it replaces ALL occurrences in one pass</li>
            </ol>

            <blockquote>
                <p>Why does order matter? Consider if we learned merges for both "th" and "he". If "th" was more frequent (learned first), we want "the" to become ["th", "e"], not ["t", "he"]. By applying merges in learned order, more frequent patterns get priority.</p>
            </blockquote>

            <h2>Decoding Tokens Back to Text</h2>

            <p>
                Decoding is the simplest operation - we just look up what each token represents:
            </p>

            <pre><code>decode(tokensToDecode) {
    const bytes = [];

    for (let i = 0; i < tokensToDecode.length; i++) {
        const token = tokensToDecode[i];

        // Look up the token's byte sequence
        const tokenBytes = this.vocabulary.get(token);
        if (tokenBytes) {
            for (let j = 0; j < tokenBytes.length; j++) {
                bytes.push(tokenBytes[j]);
            }
        } else {
            throw new Error(`Unknown token: ${token}`);
        }
    }

    return this.#bytesToString(bytes);
}</code></pre>

            <p>
                Each token ID maps to a sequence of bytes through our vocabulary. We concatenate all the byte sequences and convert back to a string. The process is deterministic and lossless - we can always recover the exact original text.
            </p>

            <p>
                Notice there's no special handling needed - every token, whether it's a single byte, a merged pair, or a special token, is just an entry in the vocabulary.
            </p>

            <h2>Handling Special Tokens</h2>

            <p>
                Special tokens like <code>&lt;|endoftext|&gt;</code> or <code>&lt;|pad|&gt;</code> need to remain intact during tokenization. We achieve this by pre-creating merge chains:
            </p>

            <pre><code>reserveToken(specialTokenString) {
    // Convert the special token to bytes
    const bytes = this.#stringToBytes(specialTokenString);
    let tokens = [...bytes]; // Copy the array

    // Create merge chain to combine all bytes into a single token
    while (tokens.length > 1) {
        // Create a merge for the first two tokens
        const newTokenId = this.#makeMerge(tokens[0], tokens[1]);

        // Apply it to our token array
        tokens = this.#applyMerge(tokens, tokens[0], tokens[1], newTokenId);
    }

    const finalTokenId = tokens[0];
    return finalTokenId;
}</code></pre>

            <p>For example, reserving <code>&lt;|pad|&gt;</code>:</p>
            <ul>
                <li>Start with bytes: <code>[60, 124, 112, 97, 100, 124, 62]</code></li>
                <li>Merge <code>(60,124)</code> -> 256</li>
                <li>Now we have: <code>[256, 112, 97, 100, 124, 62]</code></li>
                <li>Merge <code>(256,112)</code> -> 257</li>
                <li>... Continue until: <code>[261]</code> - a single token!</li>
            </ul>

            <p>
                These merges become part of the global merge rules. When encoding encounters <code>&lt;|pad|&gt;</code> in any text, the normal merge process naturally combines it into token 261. If you reserve the same special token twice, it will simply create the same merge chain (or reuse existing merges), returning the same final token ID.
            </p>

            <h1>Using the Tokenizer</h1>

            <p>Let's see our tokenizer in action with a practical example:</p>

            <pre><code>// Create and train a tokenizer
const tokenizer = new Tokenizer();

// Reserve special tokens before training
// This creates merge chains so they'll always be single tokens
tokenizer.reserveToken("<|endoftext|>");
tokenizer.reserveToken("<|pad|>");

// Train on sample text
const trainingData = `
The quick brown fox jumps over the lazy dog.
The dog was lazy but the fox was quick.
Quick foxes and lazy dogs don't mix well.
The the the - common words should merge.
`;

tokenizer.train(trainingData, 100);

// Test encoding and decoding
const testText = "The quick fox";
const encoded = tokenizer.encode(testText);
console.log(`"${testText}" -> [${encoded.join(", ")}]`);

const decoded = tokenizer.decode(encoded);
console.log(`[${encoded.join(", ")}] -> "${decoded}"`);

// Test that special tokens work correctly
const specialText = "Hello <|endoftext|> World";
const specialEncoded = tokenizer.encode(specialText);
console.log(`Special tokens remain intact: "${specialText}"`);
console.log(`Encoded as: [${specialEncoded.join(", ")}]`);

// The <|endoftext|> token appears as a single token ID in the sequence
// because we created the merge chain before training

// Verify round-trip conversion
const specialDecoded = tokenizer.decode(specialEncoded);
console.log(`Round-trip successful: ${specialText === specialDecoded}`);</code></pre>

            <p>
                The tokenizer learns patterns specific to your training data. Common words and phrases become single tokens, dramatically reducing sequence length. Special tokens like <code>&lt;|endoftext|&gt;</code> are guaranteed to remain as single tokens because we pre-created their merge chains.
            </p>

            <h1>Saving / Loading</h1>

            <p>
                You will probably want to re-use pre-made tokenizers. After all, tokenization takes time. For this reason, we should include serialize / deserialize functions in the tokenizer. The encode function returns a uint8 array of bytes. The decode function takes a uint8 array of bytes.
            </p>

            <pre><code>/**
 * Serializes the tokenizer to a binary format
 *
 * Binary format structure:
 * - Magic number (4 bytes): 0x42504531 ("BPE1")
 * - Version (4 bytes): Currently 1
 * - nextTokenId (4 bytes)
 * - Number of merges (4 bytes)
 * - For each merge:
 *   - token1 (4 bytes)
 *   - token2 (4 bytes)
 *   - mergedTokenId (4 bytes)
 * - Number of vocabulary entries (4 bytes)
 * - For each vocabulary entry:
 *   - tokenId (4 bytes)
 *   - length of byte sequence (4 bytes)
 *   - byte sequence (variable length)
 *
 * @returns {Uint8Array} Binary representation of the tokenizer
 */
serialize() {
    // Calculate required buffer size
    let bufferSize = 16; // magic (4) + version (4) + nextTokenId (4) + numMerges (4)
    bufferSize += this.merges.size * 12; // each merge: token1(4) + token2(4) + mergedId(4)
    bufferSize += 4; // numVocabulary (4)

    // Calculate vocabulary size
    for (const [tokenId, bytes] of this.vocabulary) {
        bufferSize += 8 + bytes.length; // tokenId(4) + length(4) + bytes
    }

    const buffer = new ArrayBuffer(bufferSize);
    const view = new DataView(buffer);
    const bytes = new Uint8Array(buffer);
    let offset = 0;

    // Write header
    view.setUint32(offset, 0x42504531, true); // Magic: "BPE1" in hex
    offset += 4;
    view.setUint32(offset, 1, true); // Version 1
    offset += 4;
    view.setUint32(offset, this.nextTokenId, true);
    offset += 4;

    // Write merges
    view.setUint32(offset, this.merges.size, true);
    offset += 4;

    for (const [mergeKey, mergedToken] of this.merges) {
        const [token1, token2] = mergeKey.split(',').map(Number);
        view.setUint32(offset, token1, true);
        offset += 4;
        view.setUint32(offset, token2, true);
        offset += 4;
        view.setUint32(offset, mergedToken, true);
        offset += 4;
    }

    // Write vocabulary
    view.setUint32(offset, this.vocabulary.size, true);
    offset += 4;

    for (const [tokenId, tokenBytes] of this.vocabulary) {
        view.setUint32(offset, tokenId, true);
        offset += 4;
        view.setUint32(offset, tokenBytes.length, true);
        offset += 4;
        for (let i = 0; i < tokenBytes.length; i++) {
            bytes[offset++] = tokenBytes[i];
        }
    }

    return bytes;
}

/**
 * Deserializes a tokenizer from binary format
 * @param {Uint8Array} data - Binary tokenizer data
 * @returns {Tokenizer} New tokenizer instance with loaded state
 */
static deserialize(data) {
    const view = new DataView(data.buffer, data.byteOffset, data.byteLength);
    let offset = 0;

    // Read and verify header
    const magic = view.getUint32(offset, true);
    offset += 4;
    if (magic !== 0x42504531) {
        throw new Error('Invalid tokenizer file format');
    }

    const version = view.getUint32(offset, true);
    offset += 4;
    if (version !== 1) {
        throw new Error(`Unsupported tokenizer version: ${version}`);
    }

    const tokenizer = new Tokenizer();

    // Read nextTokenId
    tokenizer.nextTokenId = view.getUint32(offset, true);
    offset += 4;

    // Read merges
    const numMerges = view.getUint32(offset, true);
    offset += 4;

    tokenizer.merges.clear();
    for (let i = 0; i < numMerges; i++) {
        const token1 = view.getUint32(offset, true);
        offset += 4;
        const token2 = view.getUint32(offset, true);
        offset += 4;
        const mergedToken = view.getUint32(offset, true);
        offset += 4;

        const mergeKey = `${token1},${token2}`;
        tokenizer.merges.set(mergeKey, mergedToken);
    }

    // Read vocabulary
    const numVocab = view.getUint32(offset, true);
    offset += 4;

    tokenizer.vocabulary.clear();
    for (let i = 0; i < numVocab; i++) {
        const tokenId = view.getUint32(offset, true);
        offset += 4;
        const length = view.getUint32(offset, true);
        offset += 4;

        const bytes = [];
        for (let j = 0; j < length; j++) {
            bytes.push(data[offset++]);
        }

        tokenizer.vocabulary.set(tokenId, bytes);
    }

    return tokenizer;
}</code></pre>

            <h1>Performance Characteristics</h1>

            <p>BPE tokenization has its trade-offs:</p>

            <p><strong>Vocabulary Size vs Sequence Length</strong>: More merges mean larger vocabulary but shorter sequences. Most modern models use 30,000-50,000 tokens as a sweet spot.</p>

            <p><strong>Training Time</strong>: O(n * m) where n is training text length and m is number of merges. Each merge requires scanning the entire token sequence.</p>

            <p><strong>Encoding Time</strong>: O(m * n) where m is the number of merges and n is the average text length after applying previous merges. We apply each merge rule exactly once in order, with each merge potentially scanning the entire sequence. This is more predictable than the iterative approach and guarantees the same tokenization as the training data.</p>

            <p><strong>Memory Usage</strong>: Vocabulary grows linearly with merges. Each merge adds one vocabulary entry and one merge rule. JavaScript Maps efficiently maintain insertion order without additional memory overhead.</p>

            <p><strong>Special Token Handling</strong>: O(s) where s is the length of the special token string. Creating merge chains is a one-time cost during reservation, after which special tokens encode with zero additional overhead - they use the same merge logic as regular text.</p>
        

            <h2>Resources</h2>

            <ul>
                <li><a href="https://gist.github.com/gszauer/270eb77c7795d7ef5d48224782f9a097">Full source code listing</a></li>
                <li><a href="tokenizer.html">HTML front end to the tokenizer (train, import, export, and test)</a></li>
            </ul>
        
        </article>
    </main>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div>&copy; 2025 <a href="https://gabormakesgames.com/">Gabor Szauer</a></div>
        </div>
    </footer>

    <script>
        // JavaScript syntax highlighting with proper tokenization
        function highlightJavaScript() {
            // Get all code blocks
            const codeBlocks = document.querySelectorAll('pre code');

            codeBlocks.forEach(block => {
                // Add language class
                block.classList.add('language-javascript');

                // Get the text content
                const code = block.textContent;

                // Tokenize the code
                const tokens = tokenizeJavaScript(code);

                // Build HTML from tokens
                let html = '';
                for (const token of tokens) {
                    if (token.type === 'plain') {
                        html += escapeHtml(token.value);
                    } else {
                        html += `<span class="${token.type}">${escapeHtml(token.value)}</span>`;
                    }
                }

                // Set the highlighted HTML
                block.innerHTML = html;
            });
        }

        function tokenizeJavaScript(code) {
            const tokens = [];
            let i = 0;

            while (i < code.length) {
                let matched = false;

                // Skip whitespace but preserve it
                if (/\s/.test(code[i])) {
                    let start = i;
                    while (i < code.length && /\s/.test(code[i])) i++;
                    tokens.push({ type: 'plain', value: code.substring(start, i) });
                    continue;
                }

                // Comments
                if (code[i] === '/' && i + 1 < code.length) {
                    if (code[i + 1] === '/') {
                        // Single line comment
                        let start = i;
                        i += 2;
                        while (i < code.length && code[i] !== '\n') i++;
                        tokens.push({ type: 'comment', value: code.substring(start, i) });
                        continue;
                    } else if (code[i + 1] === '*') {
                        // Multi-line comment
                        let start = i;
                        i += 2;
                        while (i + 1 < code.length && !(code[i] === '*' && code[i + 1] === '/')) i++;
                        if (i + 1 < code.length) i += 2;
                        tokens.push({ type: 'comment', value: code.substring(start, i) });
                        continue;
                    }
                }

                // Strings
                if (code[i] === '"' || code[i] === "'" || code[i] === '`') {
                    const quote = code[i];
                    let start = i;
                    i++;
                    while (i < code.length && code[i] !== quote) {
                        if (code[i] === '\\' && i + 1 < code.length) {
                            i += 2; // Skip escaped character
                        } else {
                            i++;
                        }
                    }
                    if (i < code.length) i++; // Include closing quote
                    tokens.push({ type: 'string', value: code.substring(start, i) });
                    continue;
                }

                // Numbers
                if (/\d/.test(code[i]) || (code[i] === '.' && i + 1 < code.length && /\d/.test(code[i + 1]))) {
                    let start = i;
                    while (i < code.length && /[\d.]/.test(code[i])) i++;
                    tokens.push({ type: 'number', value: code.substring(start, i) });
                    continue;
                }

                // Identifiers and keywords
                if (/[a-zA-Z_$#]/.test(code[i])) {
                    let start = i;
                    while (i < code.length && /[a-zA-Z0-9_$#]/.test(code[i])) i++;
                    const word = code.substring(start, i);

                    // Check if it's a keyword
                    const keywords = ['const', 'let', 'var', 'function', 'class', 'if', 'else', 'for', 'while', 'do',
                        'switch', 'case', 'break', 'continue', 'return', 'new', 'this', 'typeof', 'instanceof',
                        'try', 'catch', 'finally', 'throw', 'extends', 'implements', 'static', 'import', 'export',
                        'from', 'as', 'default', 'async', 'await', 'yield', 'of', 'in', 'debugger', 'with', 'delete', 'void'];

                    const booleans = ['true', 'false'];
                    const nullish = ['null', 'undefined'];

                    let type = 'plain';
                    if (keywords.includes(word)) {
                        type = 'keyword';
                    } else if (booleans.includes(word)) {
                        type = 'boolean';
                    } else if (word === 'null') {
                        type = 'null';
                    } else if (word === 'undefined') {
                        type = 'undefined';
                    } else {
                        // Check if it's a function call (followed by parenthesis)
                        let j = i;
                        while (j < code.length && /\s/.test(code[j])) j++;
                        if (j < code.length && code[j] === '(') {
                            type = 'function';
                        } else if (word[0] === word[0].toUpperCase() && word[0] !== word[0].toLowerCase()) {
                            // Check if previous token was 'new', 'class', or 'extends' keyword
                            let prevToken = tokens[tokens.length - 1];
                            while (prevToken && prevToken.type === 'plain' && prevToken.value.trim() === '') {
                                prevToken = tokens[tokens.length - 2];
                            }
                            if (prevToken && prevToken.type === 'keyword' &&
                                ['new', 'class', 'extends'].includes(prevToken.value)) {
                                type = 'class-name';
                            }
                        }
                    }

                    tokens.push({ type, value: word });
                    continue;
                }

                // Operators and punctuation
                const operators = [
                    '===', '!==', '==', '!=', '<=', '>=', '<<', '>>', '>>>', '+=', '-=', '*=', '/=', '%=',
                    '++', '--', '&&', '||', '<', '>', '+', '-', '*', '/', '%', '=', '!', '?', ':', '&', '|', '^', '~'
                ];

                // Try to match multi-character operators first
                let operatorMatched = false;
                for (let len = 3; len >= 1; len--) {
                    const potential = code.substring(i, i + len);
                    if (operators.includes(potential)) {
                        tokens.push({ type: 'operator', value: potential });
                        i += len;
                        operatorMatched = true;
                        break;
                    }
                }

                if (operatorMatched) continue;

                // Punctuation
                if (/[{}()\[\];,.]/.test(code[i])) {
                    tokens.push({ type: 'punctuation', value: code[i] });
                    i++;
                    continue;
                }

                // Default: treat as plain text
                tokens.push({ type: 'plain', value: code[i] });
                i++;
            }

            return tokens;
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        // Run highlighting when page loads
        document.addEventListener('DOMContentLoaded', () => {
            highlightJavaScript();
        });
    </script>
</body>
</html>
