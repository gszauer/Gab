<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Part 3: Sequences</title>
    <meta name="theme-color" content="#0A0A0F"/>
    <meta name="description" content="From embeddings and RNN cells to output layers and training loops, this guide walks through building a recurrent language model and stacking layers to create ChatRNN." />
    <style>
        :root{
            --bg:#0a0a0f;
            --bg-2:#0e0e16;
            --panel:#12121c;
            --panel-2:#171726;
            --text:#e9e9f2;
            --muted:#a5a7bf;
            --line:#1e1e2e;
            --pink:#ff2e88;
            --pink-2:#ff86c3;
            --glow: 0 0 .5rem var(--pink), 0 0 1.25rem color-mix(in oklab, var(--pink) 70%, white 0%), 0 0 2.5rem color-mix(in oklab, var(--pink) 35%, black 65%);
            --radius:16px;
            --pad: clamp(14px, 1.6vw, 22px);
            --font: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", "Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";
            --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        *{box-sizing:border-box}
        html,body{min-height:100%}
        body{
            margin:0;
            font-family:var(--font);
            color:var(--text);
            background:
                radial-gradient(1200px 520px at 50% -160px, rgba(255,46,136,.12), transparent 75%),
                linear-gradient(180deg, #090914 0%, #050509 45%, #030306 75%, #020203 100%);
            background-color:#020203;
            background-repeat:no-repeat, no-repeat;
            background-size:130% 70%, 100% 100%;
            background-position:center top, center top;
            background-attachment: fixed;
            line-height:1.6;
            overflow-x:hidden;
            -webkit-font-smoothing:antialiased;
            -moz-osx-font-smoothing:grayscale;
        }

        /* Subtle scanlines for retro vibe */
        body::before{
            content:"";
            position:fixed; inset:0;
            pointer-events:none;
            background-image: linear-gradient(rgba(255,255,255,.03),rgba(255,255,255,0) 2px);
            background-size:100% 3px; mix-blend-mode:overlay; opacity:.25;
        }
        @media (prefers-reduced-motion: reduce){
            body::before{display:none}
            *{animation: none !important; transition: none !important}
        }

        .container{width:min(1000px, 92vw); margin-inline:auto}

        /* Header */
        header{
            background: linear-gradient(180deg, #131321, #11111c);
            border-bottom: 1px solid var(--line);
            padding: 1.5rem 0;
            position: sticky;
            top: 0;
            z-index: 10;
            backdrop-filter: blur(10px);
        }

        nav{
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 1rem;
        }

        .logo{
            font-weight: 900;
            font-size: 1.25rem;
            color: var(--text);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .logo .pink{
            color: var(--pink);
            text-shadow: var(--glow);
        }

        .back-btn{
            appearance:none;
            border:1px solid color-mix(in oklab, var(--pink) 35%, #ffffff00 65%);
            color:var(--text);
            background: linear-gradient(180deg, var(--panel) 0%, var(--panel-2) 100%);
            padding:.45rem .85rem;
            border-radius: 999px;
            cursor:pointer;
            font-weight:600;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 0.35rem;
            font-size: 0.9rem;
            transition: transform .12s ease, box-shadow .15s ease;
        }

        .back-btn:hover{
            box-shadow: var(--glow);
            transform: translateY(-1px);
        }

        /* Article Hero */
        .article-hero{
            padding: 3rem 0 2rem;
            position: relative;
        }

        .grid-bg{
            position:absolute; inset:0; pointer-events:none; z-index:-1; opacity:.25;
            background:
                radial-gradient(circle at 50% -60px, color-mix(in oklab, var(--pink) 30%, transparent), transparent 35%),
                repeating-linear-gradient(0deg, #ffffff10 0 1px, transparent 1px 36px),
                repeating-linear-gradient(90deg, #ffffff10 0 1px, transparent 1px 36px);
            mask: radial-gradient(1200px 600px at 50% 0, black 40%, transparent 80%);
        }

        h1{
            font-size: clamp(32px, 5vw, 48px);
            line-height: 1.1;
            font-weight: 900;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--text) 0%, var(--pink) 100%);
            -webkit-background-clip: text;
            background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .article-meta{
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            color: var(--muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }

        .article-meta span{
            display: flex;
            align-items: center;
            gap: 0.35rem;
        }

        .article-blurb{
            font-size: 1.125rem;
            line-height: 1.6;
            color: #d9d9ec;
            max-width: 800px;
        }

        /* Article Content */
        article{
            background: linear-gradient(180deg, #131321, #11111c);
            border: 1px solid #222238;
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 2rem 0 3rem;
            box-shadow: 0 18px 50px rgba(0,0,0,.45), 0 0 60px -10px rgba(255,46,136,.15);
        }

        article h1{
            color: var(--text);
            margin: 2.5rem 0 1rem 0;
            font-size: 2.25rem;
            font-weight: 900;
            position: relative;
            padding-bottom: 0.75rem;
            background: none;
            -webkit-text-fill-color: var(--text);
        }

        article h1::after{
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 80px;
            height: 3px;
            background: linear-gradient(90deg, var(--pink), transparent);
        }

        article h1:first-child{
            margin-top: 0;
        }

        article h2{
            color: var(--text);
            margin: 2.5rem 0 1rem 0;
            font-size: 1.875rem;
            font-weight: 800;
            position: relative;
            padding-bottom: 0.75rem;
        }

        article h2::after{
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 2px;
            background: linear-gradient(90deg, var(--pink), transparent);
        }

        article h2:first-child{
            margin-top: 0;
        }

        article h3{
            color: var(--text);
            margin: 2rem 0 1rem 0;
            font-size: 1.375rem;
            font-weight: 700;
        }

        article h4{
            color: var(--text);
            margin: 1.5rem 0 0.75rem 0;
            font-size: 1.125rem;
            font-weight: 600;
        }

        article p{
            margin-bottom: 1.25rem;
            color: #d9d9ec;
            line-height: 1.7;
        }

        article ul, article ol{
            margin: 1.25rem 0;
            padding-left: 1.75rem;
            color: #d9d9ec;
        }

        article li{
            margin-bottom: 0.5rem;
            line-height: 1.7;
        }

        article strong{
            color: var(--text);
            font-weight: 600;
        }

        /* Links within article content */
        article a{
            color: var(--pink-2);
            text-decoration: none;
            border-bottom: 1px solid rgba(255, 134, 195, 0.3);
            transition: all 0.2s ease;
            font-weight: 500;
        }

        article a:hover{
            color: var(--pink);
            border-bottom-color: var(--pink);
            text-shadow: 0 0 8px rgba(255, 46, 136, 0.4);
        }

        article a:visited{
            color: color-mix(in oklab, var(--pink-2) 80%, var(--muted) 20%);
        }

        /* Code blocks */
        pre{
            background: #0f0f1b;
            border: 1px solid #2a2a3a;
            color: #e2e8f0;
            padding: 1.25rem;
            border-radius: 12px;
            margin: 1.5rem 0;
            overflow-x: auto;
            font-family: var(--mono);
            font-size: 0.875rem;
            line-height: 1.6;
            box-shadow: inset 0 2px 8px rgba(0,0,0,.3);
        }

        code{
            font-family: var(--mono);
            font-size: 0.9em;
            background: rgba(255, 46, 136, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            color: var(--pink-2);
        }

        pre code{
            background: none;
            padding: 0;
            color: #e2e8f0;
        }

        /* JavaScript Syntax Highlighting */
        .language-javascript .keyword { color: #ff79c6; font-weight: 600; }
        .language-javascript .function { color: #8be9fd; }
        .language-javascript .string { color: #f1fa8c; }
        .language-javascript .number { color: #bd93f9; }
        .language-javascript .comment { color: #6272a4; font-style: italic; }
        .language-javascript .operator { color: #ff79c6; }
        .language-javascript .class-name { color: #50fa7b; }
        .language-javascript .constant { color: #ff79c6; }
        .language-javascript .punctuation { color: #a5a7bf; }
        .language-javascript .boolean { color: #bd93f9; font-weight: 600; }
        .language-javascript .null { color: #bd93f9; font-weight: 600; }
        .language-javascript .undefined { color: #bd93f9; font-weight: 600; }

        /* Canvas diagrams */
        .diagram-container {
            background: #0a0a14;
            border: 1px solid color-mix(in oklab, var(--pink) 25%, #2a2a3a 75%);
            border-radius: 12px;
            padding: 1rem;
            margin: 1.5rem 0;
            display: flex;
            justify-content: center;
            align-items: center;
            box-shadow: inset 0 2px 8px rgba(0,0,0,.3);
        }

        .diagram-canvas {
            display: block;
            width: 100%;
            height: auto;
        }

        /* Tables */
        table{
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            background: var(--panel);
            border: 1px solid #2a2a3a;
            border-radius: 8px;
            overflow: hidden;
        }

        th{
            background: linear-gradient(180deg, color-mix(in oklab, var(--pink) 15%, var(--panel) 85%), var(--panel-2));
            color: var(--text);
            padding: 0.75rem;
            text-align: left;
            font-weight: 600;
            border-bottom: 1px solid color-mix(in oklab, var(--pink) 30%, #2a2a3a 70%);
        }

        td{
            padding: 0.75rem;
            border-bottom: 1px solid #2a2a3a;
            color: #d9d9ec;
        }

        tr:last-child td{
            border-bottom: none;
        }

        tr:hover{
            background: rgba(255, 46, 136, 0.05);
        }

        /* Blockquotes */
        blockquote{
            border-left: 3px solid var(--pink);
            background: #0a0a14;
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            border-radius: 4px;
            font-style: italic;
            color: #d9d9ec;
        }

        blockquote p{
            margin-bottom: 0.5rem;
            margin-top: 0;
        }

        blockquote p:last-child{
            margin-bottom: 0;
        }

        /* Navigation */
        .article-nav{
            display: flex;
            justify-content: space-between;
            gap: 1rem;
            margin: 3rem 0;
            padding-top: 2rem;
            border-top: 1px solid var(--line);
        }

        .nav-link{
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--pink-2);
            text-decoration: none;
            font-weight: 600;
            transition: transform 0.2s;
        }

        .nav-link:hover{
            color: var(--pink);
            transform: translateX(3px);
        }

        .nav-link.prev:hover{
            transform: translateX(-3px);
        }

        /* Footer */
        footer{
            border-top: 1px solid var(--line);
            background: #0a0a12;
            color: var(--muted);
            padding: 1rem 0 1.5rem;
            font-size: 0.875rem;
            margin-top: 4rem;
        }

        footer a{
            color: var(--pink-2);
            text-decoration: none;
            border-bottom: 1px solid rgba(255, 134, 195, 0.3);
            transition: all 0.2s ease;
        }

        footer a:hover{
            color: var(--pink);
            border-bottom-color: var(--pink);
            text-shadow: 0 0 8px rgba(255, 46, 136, 0.4);
        }

        /* Responsive */
        @media (max-width: 768px){
            article{
                padding: 1.5rem;
            }

            h1{
                font-size: 2rem;
            }

            .article-meta{
                font-size: 0.8rem;
                gap: 1rem;
            }

            pre{
                padding: 1rem;
                font-size: 0.75rem;
            }

            .diagram-container {
                padding: 1rem;
            }

            .diagram-canvas {
                width: 100%;
                height: auto;
            }
        }
    </style>
</head>
<body>
    <!-- Header/Navigation -->
    <header>
        <nav class="container">
            <a href="index.html" class="logo">
                The Gift of <span class="pink">Gab</span>
            </a>
            <a href="index.html" class="back-btn">
                Back to Home
            </a>
        </nav>
    </header>

    <!-- Main Article Content -->
    <main class="container">
        <article>
            <h1>Part 3: Sequences</h1>
            <p>Up to this point, we&#x27;ve built the foundations of neural networks: trainable neurons, dense layers, activation functions, and a BPE tokenizer. And we were able to train a network to solve XOR. But language is different. &quot;The cat sat on the mat&quot; makes sense because each word carries forward context from previous words. Our dense networks see inputs as isolated snapshots, they have no concept of sequence or flow.</p>
            <p>In this part, we&#x27;ll build a Recurrent Neural Network (RNN) that maintains memory across a sequence. We&#x27;ll combine it with our tokenizer to create ChatRNN. ChatRNN will chain together several components:</p>
            <div class="diagram-container">
                <canvas id="pipelineDiagram" class="diagram-canvas" width="900" height="120" role="img" aria-label="Flow from token IDs through embeddings, RNN layers, output logits, and softmax probabilities."></canvas>
            </div>
            <p>We already implemented a tokenizer, dense layers and activation functions. Now we need:</p>
            <ul>
                <li><strong>Embedding Layer</strong>: Converts discrete token IDs into continuous vectors</li>
                <li><strong>RNN Layer</strong>: Processes sequences while maintaining memory</li>
                <li><strong>Output Layer</strong>: Converts hidden states to vocabulary probabilities</li>
            </ul>
            <h2>The Embedding Layer</h2>
            <p>Our tokenizer produces integers,token ID 42, token ID 17. But neural networks work with continuous numbers that can be differentiated. We can&#x27;t compute the gradient of &quot;token 42.&quot;</p>
            <p>The embedding layer bridges this gap. It&#x27;s a lookup table: each token ID maps to a unique vector of floating-point numbers. These vectors start random, but through training, tokens appearing in similar contexts develop similar vectors.</p>
            <pre><code class="language-javascript">class EmbeddingLayer {
    weights = null;
    vocabSize = 0;
    embeddingDim = 0;
    cachedInputTokens = null;

    constructor(vocabSize, embeddingDim) {
        this.vocabSize = vocabSize;
        this.embeddingDim = embeddingDim;
        
        // Xavier initialization for stable gradients
        const scale = Math.sqrt(1.0 / embeddingDim);
        
        this.weights = new Array(vocabSize);
        for (let i = 0; i &lt; vocabSize; i++) {
            this.weights[i] = new Array(embeddingDim);
            for (let j = 0; j &lt; embeddingDim; j++) {
                this.weights[i][j] = (Math.random() * 2 - 1) * scale;
            }
        }
    }
}</code></pre>
            <p>The constructor creates our embedding matrix,one row per token in the vocabulary, each row containing <code>embeddingDim</code> numbers. We scale by <code>1/sqrt(embeddingDim)</code> to keep variance reasonable regardless of dimension size.</p>
            <p>Visually, a 64 dimension embedding matrix would look like this:</p>
            <div class="diagram-container">
                <canvas id="embeddingMatrixDiagram" class="diagram-canvas" width="920" height="260" role="img" aria-label="Matrix of embedding weights that maps token IDs to embedding dimensions."></canvas>
            </div>


            <h3>Embedding Forward Pass</h3>
            <p>The embedding layers forward pass converts a sequence of token IDs into a sequence of vectors. For each token, we grab its row from the embedding matrix.</p>
            <pre><code class="language-javascript">// ... class EmbeddingLayer
    forward(inputTokens) {
        // Cache for backprop,we need to know which tokens were looked up
        this.cachedInputTokens = new Array(inputTokens.length);
        for (let i = 0; i &lt; inputTokens.length; i++) {
            this.cachedInputTokens[i] = inputTokens[i];
        }
        
        // Look up each token&#x27;s embedding vector
        const output = new Array(inputTokens.length);
        for (let i = 0; i &lt; inputTokens.length; i++) {
            const tokenId = inputTokens[i];
            output[i] = new Array(this.embeddingDim);
            for (let j = 0; j &lt; this.embeddingDim; j++) {
                output[i][j] = this.weights[tokenId][j];
            }
        }
        
        return output;  // Shape: [sequenceLength][embeddingDim]
    }</code></pre>
            <p>If we input <code>[42, 17, 8]</code> with 64-dimensional embeddings, we get back three 64-element vectors. These are what the RNN will process.</p>
            <div class="diagram-container">
                <canvas id="embeddingLookupDiagram" class="diagram-canvas" width="720" height="200" role="img" aria-label="Flow of a token ID through the embedding lookup producing a vector."></canvas>
            </div>
            <h3>Embedding Backward Pass</h3>
            <p>During backpropagation, we receive gradients telling us how each embedding value should change. Unlike dense layers where every weight affects every output, embeddings are sparse: only tokens that appeared in the input get updated.</p>
            <pre><code class="language-javascript">// ... class EmbeddingLayer
    backward(outputGradients, learningRate) {
        // Only update embeddings for tokens that were actually used
        for (let i = 0; i &lt; this.cachedInputTokens.length; i++) {
            const tokenId = this.cachedInputTokens[i];
            const gradient = outputGradients[i];

            for (let j = 0; j &lt; this.embeddingDim; j++) {
                this.weights[tokenId][j] -= learningRate * gradient[j];
            }
        }
        
        // First layer,no gradients to pass back
        return null;
    }</code></pre>
            <blockquote>
                <p>If a token appears multiple times, its embedding gets updated multiple times, each occurrence contributed to the loss.</p>
            </blockquote>
            <h2>The RNN Cell</h2>
            <p>The heart of recurrence is the RNN cell. Unlike the <code>Neuron</code> class which processes inputs in isolation, an RNN cell takes TWO inputs: the current data AND a hidden state from the previous timestep. In this context, &quot;time&quot; and &quot;timestep&quot; doesn&#x27;t mean clock time, it means position in sequence. For the sentence &quot;The cat sat&quot;:</p>
            <ul>
                <li>Timestep 0 → &quot;The&quot;</li>
                <li>Timestep 1 → &quot;cat&quot;</li>
                <li>Timestep 2 → &quot;sat&quot;</li>
            </ul>
            <blockquote>
                <p>The terminology comes from RNNs originally being designed for time-series data (audio signals, stock prices, sensor readings) where each step literally was a moment in time. The naming stuck even when applying RNNs to non-temporal sequences like text.</p>
            </blockquote>
            <p>The RNN cell looks like this:</p>
            <div class="diagram-container">
                <canvas id="rnnCellDiagram" class="diagram-canvas" width="800" height="280" role="img" aria-label="Structure of an RNN cell combining current input, previous hidden state, bias, and activation."></canvas>
            </div>
            <p>The <strong>hidden state</strong> is a vector of floats — the network&#x27;s &quot;memory.&quot; If <code>hiddenSize = 128</code>, each hidden state is an array of 128 numbers. It&#x27;s a compressed representation of everything the network has seen so far in the sequence. Each new token transforms the hidden state, blending new information with the accumulated context:</p>
            <pre><code>
[0, 0, 0, ...] → &quot;The&quot; → [0.23, -0.87, ...] → &quot;cat&quot; → [-0.34, 0.91, ...] → &quot;sat&quot; → ...
    initial                after 1 token              after 2 tokens
            </code></pre>
            <blockquote>
                <p>What do these numbers mean? The network learns that through training. They might encode patterns like &quot;we&#x27;re mid-sentence&quot; or &quot;an animal was just mentioned&quot;, though in practice the representations aren&#x27;t human-interpretable. The hidden size is a hyperparameter: bigger means more capacity to remember, but more parameters to train.</p>
            </blockquote>
            
            <pre><code class="language-javascript">class RNNCell {
    weightsX = null;    // Input transformation. Shape: [inputSize][hiddenSize]
    weightsH = null;    // Hidden state. Shape: [hiddenSize][hiddenSize]
    bias = null;
    hiddenSize = 0;
    inputSize = 0;

    constructor(inputSize, hiddenSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        
        // Xavier initialization for both weight matrices
        const scaleX = Math.sqrt(2.0 / (inputSize + hiddenSize));
        const scaleH = Math.sqrt(2.0 / (hiddenSize + hiddenSize));
        
        this.weightsX = new Array(inputSize);
        for (let i = 0; i &lt; inputSize; i++) {
            this.weightsX[i] = new Array(hiddenSize);
            for (let j = 0; j &lt; hiddenSize; j++) {
                this.weightsX[i][j] = (Math.random() * 2 - 1) * scaleX;
            }
        }
        
        this.weightsH = new Array(hiddenSize);
        for (let i = 0; i &lt; hiddenSize; i++) {
            this.weightsH[i] = new Array(hiddenSize);
            for (let j = 0; j &lt; hiddenSize; j++) {
                this.weightsH[i][j] = (Math.random() * 2 - 1) * scaleH;
            }
        }
        
        this.bias = new Array(hiddenSize);
        for (let i = 0; i &lt; hiddenSize; i++) {
            this.bias[i] = 0;
        }
    }

    forward(input, prevHidden) {
        // Compute pre-activation only,activation applied separately
        const preActivation = new Array(this.hiddenSize);
        
        for (let h = 0; h &lt; this.hiddenSize; h++) {
            let sum = this.bias[h];
            
            // Add weighted input
            for (let i = 0; i &lt; this.inputSize; i++) {
                sum += input[i] * this.weightsX[i][h];
            }
            
            // Add weighted previous hidden state
            for (let i = 0; i &lt; this.hiddenSize; i++) {
                sum += prevHidden[i] * this.weightsH[i][h];
            }
            
            preActivation[h] = sum;
        }
        
        return preActivation;
    }
}</code></pre>
            <p>The forward pass combines both inputs through their respective weight matrices and adds them together with a bias. The tanh activation will be applied by a separate layer to keep values between -1 and 1.</p>


            <h2>The RNN Layer</h2>
            <p>A single RNN cell processes one timestep. To handle sequences, we wrap it in a layer that &quot;unrolls&quot; across time, meaning it processes each position in the sequence one after another. We&#x27;ll use an <code>ActivationLayer</code> for the tanh activation:</p>
            <div class="diagram-container">
                <canvas id="rnnTimelineDiagram" class="diagram-canvas" width="900" height="300" role="img" aria-label="Unrolled RNN processing tokens over timesteps with shared cell and tanh activation"></canvas>
            </div>
            <p>Each &quot;cell&quot; is the same cell reused. The hidden state flows left to right, carrying context forward.</p>
            <pre><code class="language-javascript">class RNNLayer {
    cell = null;
    activation = null;  // ActivationLayer
    hiddenSize = 0;
    
    // Caches for backpropagation through time
    cachedInputs = null;            // Shape [sequenceLength]
    cachedHiddens = null;           // Shape [sequenceLength + 1][hiddenSize]
    cachedPreActivations = null;    // Shape [sequenceLength]

    constructor(inputSize, hiddenSize) {
        this.hiddenSize = hiddenSize;
        this.cell = new RNNCell(inputSize, hiddenSize);
        
        this.activation = new ActivationLayer(&quot;tanh&quot;);
    }
}</code></pre>
            <h1>The RNN Forward Pass</h1>
            <p>The RNN forward pass is where sequence processing happens. Unlike a dense layer that sees all inputs simultaneously, the RNN processes one token at a time, maintaining a &quot;memory&quot; that flows from one timestep to the next. Imagine reading a sentence word by word—after each word, your understanding of the sentence updates. The RNN works the same way: &quot;The&quot; becomes a hidden state h₀, then &quot;The cat&quot; becomes h₁, then &quot;The cat sat&quot; becomes h₂, and so on.</p>
            <p>The <code>inputSequence</code> parameter is an array of embedding vectors. If we tokenized &quot;The cat sat&quot; and looked up embeddings, we&#x27;d have three 64-dimensional vectors, one per token. 3. Each embedding vector represents one token&#x27;s position in the learned semantic space.</p>
            <p>We create three caches before processing begins. We cache all of this because backpropagation through time needs to revisit every timestep in reverse order.</p>
            <ul>
                <li>The <code>cachedInputs</code> array stores the input embedding at each timestep.</li>
                <li>The <code>cachedHiddens</code> array stores hidden states, and it has length <code>sequenceLength + 1</code> because we need to store both the initial hidden state (all zeros, before any input) and the hidden state after each timestep.</li>
                <li>The <code>cachedPreActivations</code> array stores the values before the tanh activation is applied. </li>
            </ul> 
            <pre><code class="language-javascript">// ... class RNNLayer
    forward(inputSequence) {
        const sequenceLength = inputSequence.length;
        
        // Initialize caches
        this.cachedInputs = new Array(sequenceLength);
        this.cachedHiddens = new Array(sequenceLength + 1);
        this.cachedPreActivations = new Array(sequenceLength);
        
        // Start with zero hidden state
        this.cachedHiddens[0] = new Array(this.hiddenSize);
        for (let i = 0; i &lt; this.hiddenSize; i++) {
            this.cachedHiddens[0][i] = 0;
        }
        
        const outputs = new Array(sequenceLength);
        
        for (let t = 0; t &lt; sequenceLength; t++) {
            this.cachedInputs[t] = inputSequence[t];
            
            // Compute pre-activation using RNN cell
            const preActivation = this.cell.forward(
                inputSequence[t], 
                this.cachedHiddens[t]
            );
            this.cachedPreActivations[t] = preActivation;
            
            // Apply tanh 
            const newHidden = this.activation.forward(preActivation);
            
            this.cachedHiddens[t + 1] = newHidden;
            outputs[t] = newHidden;
        }
        
        return outputs;  // Hidden state at each timestep
    }</code></pre>
            <p>The main loop processes tokens left to right, one at a time. At each timestep <code>t</code>, we cache the current input embedding, then call <code>this.cell.forward()</code> which computes <code>Wₓ · input + Wₕ · prevHidden + bias</code>. At timestep 0, the previous hidden state is all zeros, so only the input term contributes. At subsequent timesteps, both the current input and the accumulated context from previous tokens influence the result. We then pass this pre-activation through tanh to get the new hidden state, store it at <code>cachedHiddens[t + 1]</code>, and add it to outputs.</p>
            <p>The returned <code>outputs</code> array contains the hidden state after processing each token. Each hidden state encodes cumulative context: h₁ knows about &quot;The&quot;, h₂ knows about &quot;The cat&quot;, h₃ knows about &quot;The cat sat&quot;. The same weights <code>Wₓ</code>, <code>Wₕ</code>, and <code>bias</code> are reused at every timestep,this weight sharing is what allows RNNs to handle variable-length sequences with a fixed number of parameters.</p>


            <h2>RNN Backward Pass</h2>
            <p>This is where things get interesting. We need to propagate gradients backward through time — from the last timestep to the first. This algorithm is called <strong>Backpropagation Through Time (BPTT)</strong>. Keep in mind, "time" in this context means position in sequence.</p>
            <p>Weights are shared across all timesteps. The same <code>weightsX</code>, <code>weightsH</code>, and <code>bias</code> are used at every position in the sequence. So we accumulate gradients from every timestep before making a single update.</p>
            <pre><code class="language-javascript">// ... class RNNLayer
    backward(outputGradients, learningRate) {
        const sequenceLength = this.cachedInputs.length;
        
        // Accumulate gradients across all timesteps. Initialize to 0
        const weightsXGrad = this.#createZeroMatrix(this.cell.inputSize, this.hiddenSize);
        const weightsHGrad = this.#createZeroMatrix(this.hiddenSize, this.hiddenSize);
        const biasGrad = new Array(this.hiddenSize);
        for (let i = 0; i &lt; this.hiddenSize; i++) {
            biasGrad[i] = 0;
        }

        // Gradient flowing back from future timesteps
        let hiddenGradient = new Array(this.hiddenSize);
        for (let i = 0; i &lt; this.hiddenSize; i++) {
            hiddenGradient[i] = 0;
        }
        
        const inputGradients = new Array(sequenceLength);</code></pre>
            <p><code>hiddenGradient</code> carries error from &quot;the future&quot;, timesteps we&#x27;ve already processed. At the last timestep there is no future, so it starts as zeros. <code>inputGradients</code> will hold gradients to pass back to the embedding layer.</p><p>Next, we process the sequence in reverse: last token first, first token last. This lets error flow backward through the chain of hidden states. Each hidden state receives error from two sources:</p>
            <ol>
                <li><code>outputGradients[t]</code> — how wrong was our prediction at this position?</li>
                <li><code>hiddenGradient</code> — how did this hidden state hurt predictions at <em>future</em> positions?</li>
            </ol>
            <p>We sum them because this hidden state contributed to both.</p>
            <pre><code class="language-javascript">        // Process timesteps in reverse order
        for (let t = sequenceLength - 1; t &gt;= 0; t--) {
            // Total gradient = from output layer + from future timesteps
            const totalHiddenGrad = new Array(this.hiddenSize);
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                totalHiddenGrad[h] = outputGradients[t][h] + hiddenGradient[h];
            }

            // Backprop through tanh using ActivationLayer
            this.activation.cachedInputs = this.cachedPreActivations[t];
            const preActivationGrad = this.activation.backward(totalHiddenGrad);</code></pre>
            <p>The gradient so far is with respect to the tanh <em>output</em>. We need the gradient with respect to the tanh <em>input</em> (the pre-activation). The <code>ActivationLayer</code> handles this, it uses the cached pre-activation values to compute the tanh derivative.</p>
            <p>Next we compute how each parameter contributed to the error at this timestep:</p>
            <ul>
                <li><code>weightsXGrad</code> — gradient for input-to-hidden weights, using the input at this timestep</li>
                <li><code>weightsHGrad</code> — gradient for hidden-to-hidden weights, using the <em>previous</em> hidden state</li>
                <li><code>biasGrad</code> — gradient for biases</li>
            </ul>
            <pre><code class="language-javascript">            // Accumulate weight gradients
            const input = this.cachedInputs[t];
            const prevHidden = this.cachedHiddens[t];
            
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                for (let i = 0; i &lt; this.cell.inputSize; i++) {
                    weightsXGrad[i][h] += preActivationGrad[h] * input[i];
                }
                
                for (let i = 0; i &lt; this.hiddenSize; i++) {
                    weightsHGrad[i][h] += preActivationGrad[h] * prevHidden[i];
                }
                
                biasGrad[h] += preActivationGrad[h];
            }</code></pre>
            <p>The embedding layer needs to know: &quot;how should each input value have been different?&quot; We compute this by multiplying the pre-activation gradient by <code>weightsX</code> — the same weights that transformed the input during the forward pass.</p>
            <pre><code class="language-javascript">            // Gradient to pass to embedding layer
            inputGradients[t] = new Array(this.cell.inputSize);
            for (let i = 0; i &lt; this.cell.inputSize; i++) {
                let grad = 0;
                for (let h = 0; h &lt; this.hiddenSize; h++) {
                    grad += preActivationGrad[h] * this.cell.weightsX[i][h];
                }
                inputGradients[t][i] = grad;
            }</code></pre>
            <p>This is the &quot;recurrent&quot; part of backpropagation. We compute how the <em>previous</em> hidden state contributed to error, using <code>weightsH</code>. This becomes <code>hiddenGradient</code> for the next iteration (which processes the previous timestep).</p>
            <pre><code class="language-javascript">            // Gradient to pass to previous timestep
            hiddenGradient = new Array(this.hiddenSize);
            for (let i = 0; i &lt; this.hiddenSize; i++) {
                let grad = 0;
                for (let h = 0; h &lt; this.hiddenSize; h++) {
                    grad += preActivationGrad[h] * this.cell.weightsH[i][h];
                }
                hiddenGradient[i] = grad;
            }
        }</code></pre>
            <p>After processing all timesteps, we finally update the weights. Each parameter is adjusted based on its total contribution to error across the entire sequence.</p>
            <pre><code class="language-javascript">        // Apply accumulated gradients
        for (let i = 0; i &lt; this.cell.inputSize; i++) {
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                this.cell.weightsX[i][h] -= learningRate * weightsXGrad[i][h];
            }
        }
        
        for (let i = 0; i &lt; this.hiddenSize; i++) {
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                this.cell.weightsH[i][h] -= learningRate * weightsHGrad[i][h];
            }
        }
        
        for (let h = 0; h &lt; this.hiddenSize; h++) {
            this.cell.bias[h] -= learningRate * biasGrad[h];
        }
        
        return inputGradients;
    }</code></pre>
            <p>Add a helper function to create a zero matrix, as this is going to be a common operation to perform.</p>
            <pre><code class="language-javascript">// ... class RNNLayer
    #createZeroMatrix(rows, cols) {
        const matrix = new Array(rows);
        for (let i = 0; i &lt; rows; i++) {
            matrix[i] = new Array(cols);
            for (let j = 0; j &lt; cols; j++) {
                matrix[i][j] = 0;
            }
        }
        return matrix;
    }</code></pre>
            <h2>The Output Layer</h2>
            <p>The RNN produces hidden states. We need predictions: probability distributions over the vocabulary. The output layer projects hidden states to vocabulary size, then applies softmax. The Softmax activation function is a mathematical function that converts a vector of raw numbers (called logits) into a vector of probabilities. All probabilities sum to 1.</p>
            
            <blockquote>
                <p>
                    Projecting a vector or matrix from one space or another is done with matrix multiplication. When you multiply a vector by a matrix, the output dimension is determined by the matrix shape: <code>vector [N] × matrix [N][M] = result [M]</code>
                    <ul>
                        <li>If M &lt; N: you're projecting down (compressing)</li>
                        <li>If M &gt; N: you're projecting up (expanding)</li>
                        <li>If M = N: same dimension (transforming)</li>
                    </ul>
                </p>
            </blockquote>
            
            <p>This is similar to the <code>DenseLayer</code>, but specialized for sequence classification. It computes a linear transformation followed by softmax, and integrates cross-entropy loss for cleaner gradients.</p>
            <pre><code class="language-javascript">class OutputLayer {
    weights = null;
    bias = null;
    hiddenSize = 0;
    vocabSize = 0;
    cachedInputs = null;
    cachedOutputs = null;

    constructor(hiddenSize, vocabSize) {
        this.hiddenSize = hiddenSize;
        this.vocabSize = vocabSize;
        
        const scale = Math.sqrt(2.0 / (hiddenSize + vocabSize));
        
        this.weights = new Array(hiddenSize);
        for (let i = 0; i &lt; hiddenSize; i++) {
            this.weights[i] = new Array(vocabSize);
            for (let j = 0; j &lt; vocabSize; j++) {
                this.weights[i][j] = (Math.random() * 2 - 1) * scale;
            }
        }
        
        this.bias = new Array(vocabSize);
        for (let i = 0; i &lt; vocabSize; i++) {
            this.bias[i] = 0;
        }
    }
}</code></pre>
            <h3>Softmax</h3>
            <p>The Output Layer bridges this gap. It has two distinct jobs:</p>
            <p><strong>1. </strong>Project to Vocabulary (The Logits): It transforms the 128-dimension hidden state into a 1,000-dimension vector (one number for every word in our dictionary). These raw numbers are called logits.</p>
            <ul>
                <li>A high logit (e.g., 15.5) means the network thinks this word is very likely.</li>
                <li>A low or negative logit (e.g., -5.0) means the network thinks this word is unlikely.</li>
            </ul>
            <blockquote>
                <p>Note: These are just raw scores. They don&#x27;t sum to 1, and they aren&#x27;t percentages yet.</p>
            </blockquote>
            <p><strong>2. </strong>Convert to Probabilities (Softmax): We need to turn those raw scores into probabilities so we can say, &quot;There is a 90% chance the next word is &#x27;cat&#x27; and a 1% chance it is &#x27;dog&#x27;.&quot;</p>
            <blockquote>
                <p>Softmax could be it&#x27;s own activation layer, but there is mathematical conveniance in undling the softmax activation inside the output layer</p>
            </blockquote>
            <pre><code class="language-javascript">// ... class OutputLayer
    #softmax(logits) {
        // Find max for numerical stability
        let maxLogit = logits[0];
        for (let i = 1; i &lt; logits.length; i++) {
            if (logits[i] &gt; maxLogit) {
                maxLogit = logits[i];
            }
        }
        
        // Compute exp(logit - max) and sum
        const expValues = new Array(logits.length);
        let expSum = 0;
        for (let i = 0; i &lt; logits.length; i++) {
            expValues[i] = Math.exp(logits[i] - maxLogit);
            expSum += expValues[i];
        }
        
        // Normalize
        const probabilities = new Array(logits.length);
        for (let i = 0; i &lt; logits.length; i++) {
            probabilities[i] = expValues[i] / expSum;
        }
        
        return probabilities;
    }</code></pre>
            <h3>Output Layer Forward Pass</h3>
            <p>It is the output layer&#x27;s job to convert each hidden state into a probability distribution over the entire vocabulary. This process happens in two stages.</p>
            <ul>
                <li>First, we project the hidden state to vocabulary size, if <code>hiddenSize</code> is 128 and <code>vocabSize</code> is 1000, we&#x27;re going from 128 numbers to 1000 numbers. This projection produces raw scores called logits.</li>
                <li>Second, we run softmax to convert those logits into probabilities that sum to 1.</li>
            </ul>
            <pre><code class="language-javascript">// ... class OutputLayer
    forward(hiddenSequence) {
        // Cache inputs for backpropagation
        this.cachedInputs = new Array(hiddenSequence.length);
        for (let t = 0; t &lt; hiddenSequence.length; t++) {
            this.cachedInputs[t] = new Array(this.hiddenSize);
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                this.cachedInputs[t][h] = hiddenSequence[t][h];
            }
        }
        
        this.cachedOutputs = new Array(hiddenSequence.length);
        const outputs = new Array(hiddenSequence.length);</code></pre>
            <p>We cache the inputs because backpropagation will need them. To compute weight gradients, we need to know what hidden values were multiplied by those weights during the forward pass.</p>
            <p>Now we process each timestep. The hidden state is a compressed representation of the sequence so far. We need to &quot;decode&quot; it into a score for every possible next token.</p>
            <pre><code class="language-javascript">        for (let t = 0; t &lt; hiddenSequence.length; t++) {
            const hidden = hiddenSequence[t];
            
            // Compute logits: one score per vocabulary token
            const logits = new Array(this.vocabSize);
            for (let v = 0; v &lt; this.vocabSize; v++) {
                let sum = this.bias[v];
                for (let h = 0; h &lt; this.hiddenSize; h++) {
                    sum += hidden[h] * this.weights[h][v];
                }
                logits[v] = sum;
            }</code></pre>
            <p>Each logit is a weighted sum of the hidden state values, plus a bias. The weight matrix acts as a lookup in reverse: instead of asking &quot;what does token 42 mean?&quot; , we&#x27;re asking &quot;how much does this hidden state look like it should predict token 42?&quot; High logit means the network thinks that token is likely. Low or negative logit means unlikely.</p>
            <p>The weights are learned during training. If certain hidden state patterns consistently precede token 42, the weights connecting those patterns to logit 42 will grow larger.</p>
            <pre><code class="language-javascript">            // Convert logits to probabilities
            outputs[t] = this.#softmax(logits);
            this.cachedOutputs[t] = outputs[t];
        }
        
        return outputs;  // Shape: [sequenceLength][vocabSize]
    }</code></pre>
            <p>Softmax exponentiates each logit and normalizes so the results sum to 1. This gives us a proper probability distribution—we can now say &quot;70% chance the next token is &#x27;cat&#x27;, 20% chance it&#x27;s &#x27;dog&#x27;, etc.&quot; We cache these probabilities too; the backward pass needs them to compute gradients.</p>
            <p>The returned <code>outputs</code> array contains one probability distribution per timestep. During training, we compare each distribution against the actual next token to compute loss. During generation, we sample from the final distribution to pick the next token.</p>


            <h3>Output Layer Backward Pass</h3>
            <p>When softmax and cross-entropy loss are combined, the gradient simplifies to <code>predicted - target</code>. If the network predicted 0.7 probability for the correct token, the gradient is <code>0.7 - 1 = -0.3</code>. For every wrong token with probability 0.1, the gradient is <code>0.1 - 0 = 0.1</code>. This works because cross-entropy loss is defined specifically to pair with softmax—the logarithm in cross-entropy cancels the exponential in softmax during differentiation.</p>
            <pre><code class="language-javascript">// ... class OutputLayer
    backward(targetTokens, learningRate) {
        const sequenceLength = this.cachedInputs.length;
        const hiddenGradients = new Array(sequenceLength);
        
        // Accumulate weight gradients across all timesteps
        const weightsGrad = this.#createZeroMatrix(this.hiddenSize, this.vocabSize);
        const biasGrad = new Array(this.vocabSize);
        for (let i = 0; i &lt; this.vocabSize; i++) {
            biasGrad[i] = 0;
        }</code></pre>
            <p>Like the RNN layer, we accumulate gradients before updating. The same weights transform every hidden state in the sequence, so each timestep contributes to how those weights should change.</p>
            <pre><code class="language-javascript">        for (let t = 0; t &lt; sequenceLength; t++) {
            const hidden = this.cachedInputs[t];
            const probs = this.cachedOutputs[t];
            const targetToken = targetTokens[t];</code></pre>
            <p>We retrieve three things from the forward pass: the hidden state that was input to this layer, the probability distribution the network produced, and the token that should have been predicted at this position.</p>
            <p>The target is conceptually a one-hot vector, a vector of all zeros with a single 1 at the correct token&#x27;s index. If the vocabulary has 1000 tokens and the correct answer is token 42, the target is <code>[0, 0, 0, ..., 1, ..., 0]</code> with the 1 at position 42. We want the network&#x27;s output to match this: probability 1 for the correct token, probability 0 for everything else.</p>
            <p>The gradient is <code>predicted - target</code>. For the correct token, that&#x27;s <code>probs[v] - 1</code>. If the network predicted 0.8, the gradient is -0.2, meaning &quot;you were 0.2 too low.&quot; For wrong tokens, the target is 0, so the gradient is just <code>probs[v] - 0 = probs[v]</code>. If the network gave a wrong token 0.15 probability, the gradient is +0.15, meaning &quot;you were 0.15 too high.&quot;</p>
            <pre><code class="language-javascript">            // Softmax + cross-entropy gradient: predicted - target
            const outputGrad = new Array(this.vocabSize);
            for (let v = 0; v &lt; this.vocabSize; v++) {
                if (v === targetToken) {
                    outputGrad[v] = probs[v] - 1;  // Target was 1
                } else {
                    outputGrad[v] = probs[v];      // Target was 0
                }
            }</code></pre>
            <p>Now we need to figure out which weights were responsible for the error. During the forward pass, each logit was computed as <code>sum of (hidden[h] * weights[h][v])</code>. If a logit was too high (positive gradient), we want to reduce the weights that contributed to it. If too low (negative gradient), increase them. How much? Proportional to how active the hidden unit was—if <code>hidden[h]</code> was large, that weight had more influence and needs a larger correction.</p>
            <pre><code class="language-javascript">            // Weight gradients
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                for (let v = 0; v &lt; this.vocabSize; v++) {
                    weightsGrad[h][v] += outputGrad[v] * hidden[h];
                }
            }
            
            // Bias gradients
            for (let v = 0; v &lt; this.vocabSize; v++) {
                biasGrad[v] += outputGrad[v];
            } </code></pre>
            <p>The RNN needs to know: &quot;how should your hidden states have been different?&quot; We&#x27;re passing blame backward. If a vocabulary logit <code>v</code> was wrong, every hidden unit that connected to it shares some responsibility—proportional to how strong that connection was. A hidden unit with a large positive weight to an overconfident wrong answer needs to be smaller next time.</p>
            <pre><code class="language-javascript">            // Gradient to pass back to the RNN layer
            hiddenGradients[t] = new Array(this.hiddenSize);
            for (let h = 0; h &lt; this.hiddenSize; h++) {
                let grad = 0;
                for (let v = 0; v &lt; this.vocabSize; v++) {
                    grad += outputGrad[v] * this.weights[h][v];
                }
                hiddenGradients[t][h] = grad;
            }
        }</code></pre>
            <p>After accumulating gradients from every timestep, we update the parameters:</p>
            <pre><code class="language-javascript">        // Apply accumulated gradients to weights
        for (let h = 0; h &lt; this.hiddenSize; h++) {
            for (let v = 0; v &lt; this.vocabSize; v++) {
                this.weights[h][v] -= learningRate * weightsGrad[h][v];
            }
        }
        
        // Apply accumulated gradients to biases
        for (let v = 0; v &lt; this.vocabSize; v++) {
            this.bias[v] -= learningRate * biasGrad[v];
        }
        
        return hiddenGradients;
    }</code></pre>
            <p>The returned <code>hiddenGradients</code> array has one gradient vector per timestep. These flow into the RNN&#x27;s backward pass, driving updates through the recurrent weights, and eventually back to the embedding layer.</p>
            <p>We also need the helper function to create zero-initialized matrices:</p>
            <pre><code class="language-javascript">    // ... class OutputLayer
    #createZeroMatrix(rows, cols) {
        const matrix = new Array(rows);
        for (let i = 0; i &lt; rows; i++) {
            matrix[i] = new Array(cols);
            for (let j = 0; j &lt; cols; j++) {
                matrix[i][j] = 0;
            }
        }
        return matrix;
    }</code></pre>


            <h2>Cross-Entropy Loss</h2>
            <p>We need a way to measure how wrong our predictions are. The output layer produces probability distributions over the vocabulary—one distribution per timestep. Cross-entropy loss compares each distribution against the actual next token by computing <code>-log(probability assigned to the correct token)</code>. If the network assigns 90% probability to the correct token, loss is low (0.1). If it assigns 1%, loss is high (4.6). This punishes confident wrong answers harshly, which is exactly what we want during training.</p>
            <p>The implementation loops through each timestep, looks up the probability the network assigned to the correct token, and accumulates <code>-log(prob)</code>. We add a tiny <code>epsilon</code> to prevent <code>log(0)</code> when the network assigns near-zero probability to the correct answer.</p><p>The return value is a single number: the average loss across all positions in the sequence. We use this to monitor training progress—watching it decrease over epochs tells us the network is learning.</p>
            <pre><code class="language-javascript">class Loss {
    // ... existing methods: meanSquaredError, meanAbsoluteError, binaryCrossEntropy ...
    
    // Cross-entropy for multi-class classification (like next-token prediction)
    // predictions: array of probability distributions, one per timestep
    // targetTokens: array of correct token IDs, one per timestep
    static crossEntropy(predictions, targetTokens) {
        let totalLoss = 0;
        
        for (let t = 0; t &lt; predictions.length; t++) {
            const probs = predictions[t];
            const targetToken = targetTokens[t];
            
            // -log(probability of correct token)
            const epsilon = 1e-10;  // Prevent log(0)
            totalLoss += -Math.log(probs[targetToken] + epsilon);
        }
        
        return totalLoss / predictions.length;
    }
}</code></pre>
            <p>You might notice we don&#x27;t have a separate <code>crossEntropyDerivative</code> method. When cross-entropy loss is combined with softmax, the derivative simplifies to <code>predicted - target</code>—for the correct token, <code>prob - 1</code>; for wrong tokens, <code>prob - 0</code>. We compute this directly in <code>OutputLayer.backward()</code> rather than as a separate loss function.</p>
            <h2>ChatRNN: Putting It All Together</h2>
            <p>With all our components built, can now assemble them into a language model. The <code>ChatRNN</code> class chains these layers together: token IDs flow into the embedding layer to become vectors, those vectors pass through the RNN which maintains sequential memory, and finally the output layer converts hidden states into probability distributions over our vocabulary. Training works by running a forward pass to get predictions, computing cross-entropy loss against the actual next tokens, then backpropagating gradients through all three layers in reverse order.</p>
            <pre><code class="language-javascript">class ChatRNN {
    embedding = null;
    rnn = null;
    output = null;
    vocabSize = 0;

    constructor(vocabSize, embeddingDim, hiddenSize) {
        this.vocabSize = vocabSize;
        this.embedding = new EmbeddingLayer(vocabSize, embeddingDim);
        this.rnn = new RNNLayer(embeddingDim, hiddenSize);
        this.output = new OutputLayer(hiddenSize, vocabSize);
    }

    #forward(inputTokens) {
        const embedded = this.embedding.forward(inputTokens);
        const hiddenStates = this.rnn.forward(embedded);
        const probabilities = this.output.forward(hiddenStates);
        return probabilities;
    }

    #backward(targetTokens, learningRate) {
        const hiddenGradients = this.output.backward(targetTokens, learningRate);
        const embeddingGradients = this.rnn.backward(hiddenGradients, learningRate);
        this.embedding.backward(embeddingGradients, learningRate);
    }

    train(inputTokens, targetTokens, learningRate) {
        const predictions = this.#forward(inputTokens);
        const loss = Loss.crossEntropy(predictions, targetTokens);
        this.#backward(targetTokens, learningRate);
        return loss;
    }
}</code></pre>
            <h2>Text Generation</h2>
            <p>Once trained, the model generates text by predicting one token at a time, then feeding its prediction back as input. We start with a seed sequence (even just a single token), run it through the network to get a probability distribution over the vocabulary, sample a token from that distribution, append it to our sequence, and repeat. Each iteration, the RNN&#x27;s hidden state accumulates more context, so later predictions are informed by everything generated so far.</p>
            <pre><code class="language-javascript">// ... class ChatRNN
    generate(startTokens, maxLength) {
        const generated = [];
        for (let i = 0; i &lt; startTokens.length; i++) {
            generated.push(startTokens[i]);
        }
        
        for (let i = 0; i &lt; maxLength; i++) {
            const probs = this.#forward(generated);
            const lastProbs = probs[probs.length - 1];
            const nextToken = this.#sampleFromDistribution(lastProbs);
            generated.push(nextToken);
        }
        
        return generated;
    }</code></pre>
            <p>The <code>#sampleFromDistribution</code> method performs weighted random selection. Given an array of probabilities, it returns an index where higher-probability entries are proportionally more likely to be chosen. The algorithm uses the inverse transform method. Imagine laying out the probabilities as segments on a number line from 0 to 1. If token 0 has probability 0.1, it occupies (0, 0.1). If token 1 has probability 0.25, it occupies [0.1, 0.35]. And so on. We generate a uniform random number between 0 and 1, then find which segment it lands in.</p>
            <pre><code class="language-javascript">// ... class ChatRNN
    #sampleFromDistribution(probs) {
        const random = Math.random();
        let cumulative = 0;
        
        for (let i = 0; i &lt; probs.length; i++) {
            cumulative += probs[i];
            if (random &lt; cumulative) {
                return i;
            }
        }
        
        return probs.length - 1;
    }</code></pre>
            <blockquote>
                <p>Why sample instead of just picking the highest probability token? Sampling introduces controlled randomness. If the model thinks &quot;cat&quot; has 40% probability and &quot;dog&quot; has 35%, always picking &quot;cat&quot; would make generation deterministic and repetitive. Sampling lets &quot;dog&quot; win sometimes, producing more varied and interesting output. The probabilities still matter—&quot;cat&quot; will be chosen more often than &quot;dog&quot;—but rare tokens get a chance too.</p>
            </blockquote>
            <h2>Training with the BPE Tokenizer</h2>
            <p>Now we can use the <code>Tokenizer</code> class from to train our language model. The tokenizer handles converting text to tokens and back:</p>
            <pre><code class="language-javascript">// Load training text
const trainingText = `Hello world. Hello there. Hello friend. 
The quick brown fox jumps over the lazy dog.
The dog was lazy but the fox was quick.`;

// Create and train tokenizer
const tokenizer = new Tokenizer();
tokenizer.train(trainingText, 100);

console.log(`Vocabulary size: ${tokenizer.vocabulary.size}`);

// Tokenize training data
const tokens = tokenizer.encode(trainingText);
console.log(`Training tokens: ${tokens.length}`);

// Create model
const embeddingDim = 64;
const hiddenSize = 128;
const model = new ChatRNN(tokenizer.vocabulary.size, embeddingDim, hiddenSize);

// Training parameters
const learningRate = 0.1;
const epochs = 500;
const sequenceLength = 32;
const slideStep = 16;

// Training loop
for (let epoch = 0; epoch &lt; epochs; epoch++) {
    let totalLoss = 0;
    let batchCount = 0;
    
    // Slide window across training data
    for (let start = 0; start &lt; tokens.length - sequenceLength - 1; start += slideStep) {
        const inputTokens = tokens.slice(start, start + sequenceLength);
        const targetTokens = tokens.slice(start + 1, start + sequenceLength + 1);
        
        const loss = model.train(inputTokens, targetTokens, learningRate);
        totalLoss += loss;
        batchCount++;
    }
    
    if (epoch % 50 === 0) {
        const avgLoss = totalLoss / batchCount;
        console.log(`Epoch ${epoch}: Loss = ${avgLoss.toFixed(4)}`);
        
        // Generate sample
        const prompt = tokenizer.encode(&quot;Hello&quot;);
        const generated = model.generate(prompt, 20);
        const text = tokenizer.decode(generated);
        console.log(`  Sample: &quot;${text}&quot;`);
    }
}

// Final generation
console.log(&quot;\n=== Generation ===&quot;);
const prompt = tokenizer.encode(&quot;The &quot;);
const generated = model.generate(prompt, 50);
console.log(tokenizer.decode(generated)); </code></pre>
            <p>The training loop:</p>
            <ol>
                <li>Slides a window across tokenized text</li>
                <li>Input is positions 0 to N-1, target is positions 1 to N (next token prediction)</li>
                <li>Runs forward pass, computes loss, runs backward pass</li>
                <li>Periodically generates samples to monitor progress</li>
            </ol>
            <h2>Stacking RNN Layers</h2>
            <p>A single RNN layer captures patterns, but stacking layers creates hierarchy,early layers learn simple patterns, deeper layers combine them into complex concepts.</p>
            <h3>RNN Layer with Residual Connections</h3>
            <p>Deep networks face a problem: gradients must travel backward through every layer during training, and with each layer the signal can shrink or distort. Residual connections solve this by creating a shortcut, we add the original input directly to the output, giving gradients a &quot;highway&quot; that bypasses the complex transformation. This only works when input and output dimensions match.</p>
            <pre><code class="language-javascript">class RNNLayerWithResidual {
    rnn = null;
    inputSize = 0;
    hiddenSize = 0;
    canUseResidual = false;

    constructor(inputSize, hiddenSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.rnn = new RNNLayer(inputSize, hiddenSize);
        this.canUseResidual = (inputSize === hiddenSize);
    }</code></pre>
            <p>The forward pass runs the input through the RNN normally, then adds the original input back to the output when dimensions allow. Each output value becomes the sum of the RNN&#x27;s transformation plus the raw input signal.</p>
            <pre><code class="language-javascript">// ... class RNNLayerWithResidual
    forward(inputSequence) {
        const rnnOutput = this.rnn.forward(inputSequence);
        
        if (this.canUseResidual) {
            for (let t = 0; t &lt; rnnOutput.length; t++) {
                for (let h = 0; h &lt; this.hiddenSize; h++) {
                    rnnOutput[t][h] += inputSequence[t][h];
                }
            }
        }
        
        return rnnOutput;
    }</code></pre>
            <p>The backward pass mirrors this. We backpropagate through the RNN to get input gradients, then add the output gradients directly to them. This reflects that during forward, the input contributed through two paths—the RNN transformation and the direct addition—so gradients must flow back through both.</p>
            <pre><code class="language-javascript">// ... class RNNLayerWithResidual
    backward(outputGradients, learningRate) {
        const inputGradients = this.rnn.backward(outputGradients, learningRate);
        
        if (this.canUseResidual) {
            for (let t = 0; t &lt; outputGradients.length; t++) {
                for (let h = 0; h &lt; this.hiddenSize; h++) {
                    inputGradients[t][h] += outputGradients[t][h];
                }
            }
        }
        
        return inputGradients;
    }
}</code></pre>
            <h3>Multi-Layer ChatRNN</h3>
            <p>To build a deeper ChatRNN, you&#x27;d stack multiple <code>RNNLayerWithResidual</code> instances between the embedding and output layers. The first layer transforms from <code>embeddingDim</code> to <code>hiddenSize</code>, then subsequent layers maintain <code>hiddenSize</code> throughout, enabling residual connections at every level.</p>
            <pre><code class="language-javascript">class ChatRNN {
    embedding = null;
    rnnLayers = null;
    output = null;
    vocabSize = 0;
    numLayers = 0;

    constructor(vocabSize, embeddingDim, hiddenSize, numLayers = 1) {
        this.vocabSize = vocabSize;
        this.numLayers = numLayers;
        
        this.embedding = new EmbeddingLayer(vocabSize, embeddingDim);
        
        // Stack RNN layers
        this.rnnLayers = new Array(numLayers);
        for (let i = 0; i &lt; numLayers; i++) {
            const inputSize = (i === 0) ? embeddingDim : hiddenSize;
            this.rnnLayers[i] = new RNNLayerWithResidual(inputSize, hiddenSize);
        }
        
        this.output = new OutputLayer(hiddenSize, vocabSize);
    }

    forward(inputTokens) {
        let layerInput = this.embedding.forward(inputTokens);
        
        for (let i = 0; i &lt; this.rnnLayers.length; i++) {
            layerInput = this.rnnLayers[i].forward(layerInput);
        }
        
        return this.output.forward(layerInput);
    }

    backward(targetTokens, learningRate) {
        let gradients = this.output.backward(targetTokens, learningRate);
        
        for (let i = this.rnnLayers.length - 1; i &gt;= 0; i--) {
            gradients = this.rnnLayers[i].backward(gradients, learningRate);
        }
        
        this.embedding.backward(gradients, learningRate);
    }

    // ... train() and generate() same as before
}</code></pre>
            <p>Now you can create deeper models:</p>
            <pre><code class="language-javascript">// 3-layer RNN
const model = new ChatRNN(vocabSize, 64, 128, 3); </code></pre>
            <h2>The Vanishing Gradient Problem</h2>
            <p>There&#x27;s a fundamental limitation with vanilla RNNs. During backpropagation through time, gradients get multiplied at each step. If multipliers are consistently &lt; 1, gradients shrink exponentially:</p>
            <ul>
                <li>After 10 steps: 0.9^10 ≈ 0.35</li>
                <li>After 50 steps: 0.9^50 ≈ 0.005</li>
                <li>After 100 steps: 0.9^100 ≈ 0.00003</li>
            </ul>
            <p>The gradient essentially vanishes. Early timesteps receive almost no learning signal,they stop learning while later timesteps train normally. This is why vanilla RNNs struggle with sequences longer than ~20-30 tokens. Solutions include:</p>
            <ul>
                <li><strong>Gradient clipping</strong>: Cap gradient magnitude</li>
                <li><strong>Residual connections</strong>: Provide gradient highways (what we implemented)</li>
                <li><strong>LSTM/GRU</strong>: Explicit gating mechanisms</li>
                <li><strong>Transformers</strong>: Attention bypasses sequential propagation</li>
            </ul>

            <h2>Resources</h2>

            <ul>
                <li><a href="https://gist.github.com/gszauer/bfab9bc9dbbf232fac7beee43f8cabd7">Full source code listing</a></li>
                <li><a href="rnn_trainer.html">HTML Trainer</a></li>
            </ul>

        </article>
    </main>

    <!-- Footer -->
     <footer>
    <div class="container" style="display:flex; justify-content:space-between; align-items:center; flex-wrap:wrap; gap:8px;">
      <div>&copy; 2025 <a href="https://gabormakesgames.com/">Gabor Szauer</a></div>
      <div style="display:flex; gap:1rem;">
        <a href="WebGL_Inference.md">WebGL_Inference.md</a>
        <a href="https://github.com/gszauer/Gab">GitHub</a>
        <a href="https://bsky.app/profile/gszauer.bsky.social">bsky</a>
      </div>
    </div>
  </footer>

    <script>
        // JavaScript syntax highlighting with proper tokenization
        function highlightJavaScript() {
            // Get all code blocks except ASCII diagrams
            const codeBlocks = document.querySelectorAll('pre:not(.ascii-diagram) code');

            codeBlocks.forEach(block => {
                // Add language class
                block.classList.add('language-javascript');

                // Get the text content
                const code = block.textContent;

                // Tokenize the code
                const tokens = tokenizeJavaScript(code);

                // Build HTML from tokens
                let html = '';
                for (const token of tokens) {
                    if (token.type === 'plain') {
                        html += escapeHtml(token.value);
                    } else {
                        html += `<span class="${token.type}">${escapeHtml(token.value)}</span>`;
                    }
                }

                // Set the highlighted HTML
                block.innerHTML = html;
            });
        }

        function tokenizeJavaScript(code) {
            const tokens = [];
            let i = 0;

            while (i < code.length) {
                let matched = false;

                // Skip whitespace but preserve it
                if (/\s/.test(code[i])) {
                    let start = i;
                    while (i < code.length && /\s/.test(code[i])) i++;
                    tokens.push({ type: 'plain', value: code.substring(start, i) });
                    continue;
                }

                // Comments
                if (code[i] === '/' && i + 1 < code.length) {
                    if (code[i + 1] === '/') {
                        // Single line comment
                        let start = i;
                        i += 2;
                        while (i < code.length && code[i] !== '\n') i++;
                        tokens.push({ type: 'comment', value: code.substring(start, i) });
                        continue;
                    } else if (code[i + 1] === '*') {
                        // Multi-line comment
                        let start = i;
                        i += 2;
                        while (i + 1 < code.length && !(code[i] === '*' && code[i + 1] === '/')) i++;
                        if (i + 1 < code.length) i += 2;
                        tokens.push({ type: 'comment', value: code.substring(start, i) });
                        continue;
                    }
                }

                // Strings
                if (code[i] === '"' || code[i] === "'" || code[i] === '`') {
                    const quote = code[i];
                    let start = i;
                    i++;
                    while (i < code.length && code[i] !== quote) {
                        if (code[i] === '\\' && i + 1 < code.length) {
                            i += 2; // Skip escaped character
                        } else {
                            i++;
                        }
                    }
                    if (i < code.length) i++; // Include closing quote
                    tokens.push({ type: 'string', value: code.substring(start, i) });
                    continue;
                }

                // Numbers
                if (/\d/.test(code[i]) || (code[i] === '.' && i + 1 < code.length && /\d/.test(code[i + 1]))) {
                    let start = i;
                    while (i < code.length && /[\d.]/.test(code[i])) i++;
                    tokens.push({ type: 'number', value: code.substring(start, i) });
                    continue;
                }

                // Identifiers and keywords
                if (/[a-zA-Z_$]/.test(code[i])) {
                    let start = i;
                    while (i < code.length && /[a-zA-Z0-9_$]/.test(code[i])) i++;
                    const word = code.substring(start, i);

                    // Check if it's a keyword
                    const keywords = ['const', 'let', 'var', 'function', 'class', 'if', 'else', 'for', 'while', 'do',
                        'switch', 'case', 'break', 'continue', 'return', 'new', 'this', 'typeof', 'instanceof',
                        'try', 'catch', 'finally', 'throw', 'extends', 'implements', 'static', 'import', 'export',
                        'from', 'as', 'default', 'async', 'await', 'yield', 'of', 'in', 'debugger', 'with', 'delete', 'void'];

                    const booleans = ['true', 'false'];
                    const nullish = ['null', 'undefined'];

                    let type = 'plain';
                    if (keywords.includes(word)) {
                        type = 'keyword';
                    } else if (booleans.includes(word)) {
                        type = 'boolean';
                    } else if (word === 'null') {
                        type = 'null';
                    } else if (word === 'undefined') {
                        type = 'undefined';
                    } else {
                        // Check if it's a function call (followed by parenthesis)
                        let j = i;
                        while (j < code.length && /\s/.test(code[j])) j++;
                        if (j < code.length && code[j] === '(') {
                            type = 'function';
                        } else if (word[0] === word[0].toUpperCase() && word[0] !== word[0].toLowerCase()) {
                            // Check if previous token was 'new', 'class', or 'extends' keyword
                            let prevToken = tokens[tokens.length - 1];
                            while (prevToken && prevToken.type === 'plain' && prevToken.value.trim() === '') {
                                prevToken = tokens[tokens.length - 2];
                            }
                            if (prevToken && prevToken.type === 'keyword' &&
                                ['new', 'class', 'extends'].includes(prevToken.value)) {
                                type = 'class-name';
                            }
                        }
                    }

                    tokens.push({ type, value: word });
                    continue;
                }

                // Operators and punctuation
                const operators = [
                    '===', '!==', '==', '!=', '<=', '>=', '<<', '>>', '>>>', '+=', '-=', '*=', '/=', '%=',
                    '++', '--', '&&', '||', '<', '>', '+', '-', '*', '/', '%', '=', '!', '?', ':', '&', '|', '^', '~'
                ];

                // Try to match multi-character operators first
                let operatorMatched = false;
                for (let len = 3; len >= 1; len--) {
                    const potential = code.substring(i, i + len);
                    if (operators.includes(potential)) {
                        tokens.push({ type: 'operator', value: potential });
                        i += len;
                        operatorMatched = true;
                        break;
                    }
                }

                if (operatorMatched) continue;

                // Punctuation
                if (/[{}()\[\];,.]/.test(code[i])) {
                    tokens.push({ type: 'punctuation', value: code[i] });
                    i++;
                    continue;
                }

                // Default: treat as plain text
                tokens.push({ type: 'plain', value: code[i] });
                i++;
            }

            return tokens;
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        // Run highlighting when page loads
        document.addEventListener('DOMContentLoaded', () => {
            highlightJavaScript();
            drawPipelineDiagram();
            drawEmbeddingMatrixDiagram();
            drawEmbeddingLookupDiagram();
            drawRnnCellDiagram();
            drawRnnTimelineDiagram();
        });

        const diagramPalette = {
            background: '#0a0a14',
            panel: '#12121c',
            accent: '#ff2e88',
            accentSoft: '#ff86c3',
            text: '#e9e9f2',
            muted: '#a5a7bf',
            line: '#2a2a3a'
        };

        function setupCanvas(canvas) {
            const dpr = window.devicePixelRatio || 1;
            // Use the original HTML attributes for logical dimensions
            const logicalWidth = parseInt(canvas.getAttribute('width'), 10);
            const logicalHeight = parseInt(canvas.getAttribute('height'), 10);
            // Set canvas buffer size for crisp rendering on high-DPI displays
            canvas.width = logicalWidth * dpr;
            canvas.height = logicalHeight * dpr;
            const ctx = canvas.getContext('2d');
            ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
            // Set CSS dimensions to match logical size; CSS width:100% will scale it down
            canvas.style.width = logicalWidth + 'px';
            canvas.style.height = logicalHeight + 'px';
            canvas.style.maxWidth = '100%';
            canvas.style.height = 'auto';
            return { ctx, width: logicalWidth, height: logicalHeight };
        }

        function drawRoundedRect(ctx, x, y, width, height, radius, fillStyle, strokeStyle) {
            const r = Math.min(radius, width / 2, height / 2);
            ctx.beginPath();
            ctx.moveTo(x + r, y);
            ctx.lineTo(x + width - r, y);
            ctx.quadraticCurveTo(x + width, y, x + width, y + r);
            ctx.lineTo(x + width, y + height - r);
            ctx.quadraticCurveTo(x + width, y + height, x + width - r, y + height);
            ctx.lineTo(x + r, y + height);
            ctx.quadraticCurveTo(x, y + height, x, y + height - r);
            ctx.lineTo(x, y + r);
            ctx.quadraticCurveTo(x, y, x + r, y);
            ctx.closePath();
            if (fillStyle) {
                ctx.fillStyle = fillStyle;
                ctx.fill();
            }
            if (strokeStyle) {
                ctx.strokeStyle = strokeStyle;
                ctx.stroke();
            }
        }

        function drawArrow(ctx, fromX, fromY, toX, toY, color = diagramPalette.accentSoft, lineWidth = 2) {
            ctx.save();
            ctx.strokeStyle = color;
            ctx.lineWidth = lineWidth;
            ctx.beginPath();
            ctx.moveTo(fromX, fromY);
            ctx.lineTo(toX, toY);
            ctx.stroke();

            const angle = Math.atan2(toY - fromY, toX - fromX);
            const headLength = 10;
            ctx.beginPath();
            ctx.moveTo(toX, toY);
            ctx.lineTo(
                toX - headLength * Math.cos(angle - Math.PI / 6),
                toY - headLength * Math.sin(angle - Math.PI / 6)
            );
            ctx.lineTo(
                toX - headLength * Math.cos(angle + Math.PI / 6),
                toY - headLength * Math.sin(angle + Math.PI / 6)
            );
            ctx.closePath();
            ctx.fillStyle = color;
            ctx.fill();
            ctx.restore();
        }

        function drawPipelineDiagram() {
            const canvas = document.getElementById('pipelineDiagram');
            if (!canvas) return;

            const { ctx, width, height } = setupCanvas(canvas);
            ctx.clearRect(0, 0, width, height);
            ctx.fillStyle = diagramPalette.background;
            ctx.fillRect(0, 0, width, height);

            const stages = [
                { title: 'Token IDs', bottom: '[42, 17]' },
                { title: 'Embedding', bottom: 'Vectors for each token' },
                { title: 'RNN Layers', bottom: 'Process with sequential memory' },
                { title: 'Output', bottom: 'Next-token predictions' },
                { title: 'Probabilities', bottom: 'Softmax output' }
            ];
            ctx.textAlign = 'center';

            const boxWidth = 150;
            const boxHeight = 60;
            const gap = 30;
            const totalWidth = stages.length * boxWidth + (stages.length - 1) * gap;
            const startX = (width - totalWidth) / 2;
            const labelSpacing = 16;
            const boxY = (height - (boxHeight + labelSpacing)) / 2;

            stages.forEach((stage, index) => {
                const x = startX + index * (boxWidth + gap);
                drawRoundedRect(ctx, x, boxY, boxWidth, boxHeight, 12, diagramPalette.panel, diagramPalette.line);

                ctx.fillStyle = diagramPalette.text;
                ctx.font = 'bold 16px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
                ctx.fillText(stage.title, x + boxWidth / 2, boxY + boxHeight / 2 + 6);

                ctx.fillStyle = diagramPalette.muted;
                const isIds = stage.bottom.startsWith('[');
                ctx.font = isIds
                    ? '13px ui-monospace, SFMono-Regular, Menlo, monospace'
                    : '12px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
                ctx.fillText(stage.bottom, x + boxWidth / 2, boxY + boxHeight + labelSpacing);

                if (index < stages.length - 1) {
                    const nextX = x + boxWidth + gap;
                    drawArrow(ctx, x + boxWidth, boxY + boxHeight / 2, nextX - 10, boxY + boxHeight / 2);
                }
            });
        }

        function drawEmbeddingMatrixDiagram() {
            const canvas = document.getElementById('embeddingMatrixDiagram');
            if (!canvas) return;

            const { ctx, width, height } = setupCanvas(canvas);
            ctx.clearRect(0, 0, width, height);
            ctx.fillStyle = diagramPalette.background;
            ctx.fillRect(0, 0, width, height);

            const columns = ['dim 0', 'dim 1', 'dim 2', 'dim 3', 'dim 4', '…', 'dim 63'];
            const rows = [
                { id: '0', values: ['0.12', '-0.34', '0.07', '0.91', '-0.23', '…', '0.45'], token: '"the"' },
                { id: '1', values: ['-0.56', '0.23', '0.44', '-0.18', '0.67', '…', '-0.31'], token: '"cat"' },
                { id: '2', values: ['0.89', '0.02', '-0.61', '0.33', '-0.45', '…', '0.12'], token: '"dog"' },
                { id: '…', values: ['…', '…', '…', '…', '…', '…', '…'], token: '…' },
                { id: '42', values: ['0.34', '-0.78', '0.15', '0.52', '-0.09', '…', '0.67'], token: '"sat"' }
            ];

            const labelWidth = 60;
            const tokenWidth = 140;
            const columnWidth = 90;
            const headerHeight = 32;
            const rowHeight = 34;
            const tableX = 30;
            const panelGap = 16;
            const matrixWidth = columns.length * columnWidth;
            const matrixHeight = headerHeight + rows.length * rowHeight;

            const tableY = (height - matrixHeight) / 2;
            const labelX = tableX;
            const matrixX = labelX + labelWidth + panelGap;
            const tokenPanelX = matrixX + matrixWidth + panelGap;

            drawRoundedRect(ctx, labelX, tableY, labelWidth, matrixHeight, 12, '#171726', diagramPalette.line);
            drawRoundedRect(ctx, matrixX, tableY, matrixWidth, matrixHeight, 12, diagramPalette.panel, diagramPalette.line);
            drawRoundedRect(ctx, tokenPanelX, tableY, tokenWidth, matrixHeight, 12, '#171726', diagramPalette.line);

            ctx.fillStyle = '#171726';
            ctx.fillRect(matrixX, tableY, matrixWidth, headerHeight);

            ctx.strokeStyle = diagramPalette.line;
            ctx.lineWidth = 1;
            ctx.strokeRect(matrixX, tableY, matrixWidth, matrixHeight);

            for (let i = 0; i <= rows.length; i++) {
                const y = tableY + headerHeight + i * rowHeight;
                ctx.beginPath();
                ctx.moveTo(matrixX, y);
                ctx.lineTo(matrixX + matrixWidth, y);
                ctx.stroke();
            }

            for (let i = 0; i <= columns.length; i++) {
                const x = matrixX + i * columnWidth;
                ctx.beginPath();
                ctx.moveTo(x, tableY);
                ctx.lineTo(x, tableY + matrixHeight);
                ctx.stroke();
            }

            ctx.fillStyle = diagramPalette.text;
            ctx.font = '12px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
            ctx.textAlign = 'center';
            ctx.fillText('Token ID', labelX + labelWidth / 2, tableY + headerHeight / 2 + 4);
            ctx.fillText('Token Text', tokenPanelX + tokenWidth / 2, tableY + headerHeight / 2 + 4);
            for (let i = 0; i < columns.length; i++) {
                const xCenter = matrixX + i * columnWidth + columnWidth / 2;
                ctx.fillText(columns[i], xCenter, tableY + headerHeight / 2 + 4);
            }

            ctx.font = '13px ui-monospace, SFMono-Regular, Menlo, monospace';
            rows.forEach((row, rowIndex) => {
                const y = tableY + headerHeight + rowHeight * rowIndex + rowHeight / 2 + 5;

                ctx.fillStyle = diagramPalette.text;
                ctx.fillText(row.id, labelX + labelWidth / 2, y);

                ctx.fillStyle = diagramPalette.muted;
                for (let i = 0; i < columns.length; i++) {
                    const xCenter = matrixX + i * columnWidth + columnWidth / 2;
                    ctx.fillText(row.values[i], xCenter, y);
                }

                ctx.fillStyle = diagramPalette.text;
                ctx.fillText(row.token, tokenPanelX + tokenWidth / 2, y);
            });

        }

        function drawEmbeddingLookupDiagram() {
            const canvas = document.getElementById('embeddingLookupDiagram');
            if (!canvas) return;

            const { ctx, width, height } = setupCanvas(canvas);
            ctx.clearRect(0, 0, width, height);
            ctx.fillStyle = diagramPalette.background;
            ctx.fillRect(0, 0, width, height);

            ctx.textAlign = 'center';
            ctx.fillStyle = diagramPalette.text;
            ctx.font = '16px ui-monospace, SFMono-Regular, Menlo, monospace';
            const centerX = width / 2;

            let y = 28;
            ctx.fillText('Input: token ID 42 ("sat")', centerX, y);

            drawArrow(ctx, centerX, y + 8, centerX, y + 34, diagramPalette.accentSoft, 2);
            y += 55;

            ctx.fillText('Look up row 42 in the matrix', centerX, y);

            drawArrow(ctx, centerX, y + 8, centerX, y + 34, diagramPalette.accentSoft, 2);
            y += 55;

            ctx.fillText('Output: [0.34, -0.78, 0.15, 0.52, -0.09, ..., 0.67]', centerX, y);

            ctx.textAlign = 'left';
            ctx.font = '12px ui-monospace, SFMono-Regular, Menlo, monospace';
            ctx.fillStyle = diagramPalette.text;
            const outputText = 'Output: [0.34, -0.78, 0.15, 0.52, -0.09, ..., 0.67]';
            const leftEdge = centerX - ctx.measureText(outputText).width / 2;
            let bracketStart = leftEdge + ctx.measureText('Output: ').width;
            const bracketEnd = leftEdge + ctx.measureText('Output: [0.34, -0.78, 0.15, 0.52, -0.09, ..., 0.67]').width + 60;
            bracketStart -= 40;

            ctx.beginPath();
            ctx.moveTo(bracketStart, y + 16);
            ctx.lineTo(bracketEnd, y + 16);
            ctx.strokeStyle = diagramPalette.text;
            ctx.lineWidth = 1.5;
            ctx.stroke();

            const underlineCenter = (bracketStart + bracketEnd) / 2;
            ctx.textAlign = 'center';
            ctx.fillText('embeddingDim dimensional vector', underlineCenter, y + 32);
            ctx.fillStyle = diagramPalette.muted;
            ctx.fillText('embeddingDim = 64 in this example', underlineCenter, y + 48);
        }

        function drawRnnCellDiagram() {
            const canvas = document.getElementById('rnnCellDiagram');
            if (!canvas) return;

            const { ctx, width, height } = setupCanvas(canvas);
            ctx.clearRect(0, 0, width, height);
            ctx.fillStyle = diagramPalette.background;
            ctx.fillRect(0, 0, width, height);

            const frame = { x: 140, y: 30, width: Math.min(470, width - 260), height: 210 };
            drawRoundedRect(ctx, frame.x, frame.y, frame.width, frame.height, 18, '#11111c', diagramPalette.line);

            const wxRect = { x: frame.x + 55, y: frame.y + 35, width: 110, height: 50 };
            const whRect = { x: frame.x + 55, y: frame.y + frame.height - 35 - 50, width: 110, height: 50 };
            const joinX = wxRect.x + wxRect.width + 35;
            const biasRect = { x: joinX + 30, y: frame.y + frame.height / 2 - 22, width: 120, height: 45 };
            drawRoundedRect(ctx, wxRect.x, wxRect.y, wxRect.width, wxRect.height, 10, '#171726', diagramPalette.line);
            drawRoundedRect(ctx, whRect.x, whRect.y, whRect.width, whRect.height, 10, '#171726', diagramPalette.line);
            drawRoundedRect(ctx, biasRect.x, biasRect.y, biasRect.width, biasRect.height, 10, '#171726', diagramPalette.line);

            ctx.fillStyle = diagramPalette.text;
            ctx.font = '16px ui-monospace, SFMono-Regular, Menlo, monospace';
            ctx.textAlign = 'center';
            ctx.fillText('Wx', wxRect.x + wxRect.width / 2, wxRect.y + 32);
            ctx.fillText('Wh', whRect.x + whRect.width / 2, whRect.y + 32);
            ctx.fillText('+ bias', biasRect.x + biasRect.width / 2, biasRect.y + 28);

            ctx.font = '14px ui-monospace, SFMono-Regular, Menlo, monospace';
            ctx.textAlign = 'right';
            ctx.fillText('Current Input', frame.x - 10, wxRect.y + wxRect.height / 2 - 6);
            ctx.fillText('(xₜ)', frame.x - 10, wxRect.y + wxRect.height / 2 + 14);
            ctx.fillText('Prev Hidden', frame.x - 10, whRect.y + whRect.height / 2 - 6);
            ctx.fillText('(hₜ₋₁)', frame.x - 10, whRect.y + whRect.height / 2 + 14);

            drawArrow(ctx, frame.x - 5, wxRect.y + wxRect.height / 2, wxRect.x, wxRect.y + wxRect.height / 2,
                diagramPalette.accentSoft);
            drawArrow(ctx, frame.x - 5, whRect.y + whRect.height / 2, whRect.x, whRect.y + whRect.height / 2,
                diagramPalette.accentSoft);

            const topY = wxRect.y + wxRect.height / 2;
            const bottomY = whRect.y + whRect.height / 2;
            ctx.strokeStyle = diagramPalette.text;
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(joinX, topY);
            ctx.lineTo(joinX, bottomY);
            ctx.stroke();

            ctx.beginPath();
            ctx.moveTo(wxRect.x + wxRect.width, topY);
            ctx.lineTo(joinX, topY);
            ctx.stroke();
            ctx.beginPath();
            ctx.moveTo(whRect.x + whRect.width, bottomY);
            ctx.lineTo(joinX, bottomY);
            ctx.stroke();

            const midY = (topY + bottomY) / 2;
            drawArrow(ctx, joinX, midY, biasRect.x, biasRect.y + biasRect.height / 2, diagramPalette.text);

            const preActX = biasRect.x + biasRect.width + 30;
            drawArrow(ctx, biasRect.x + biasRect.width, biasRect.y + biasRect.height / 2, preActX, midY,
                diagramPalette.text);

            ctx.textAlign = 'left';
            ctx.font = '15px ui-monospace, SFMono-Regular, Menlo, monospace';
            const preActLabelX = preActX + 10;
            const preActLabelY = midY + 5;
            ctx.fillText('preAct', preActLabelX, preActLabelY);

            const labelWidth = ctx.measureText('preAct').width;
            const arrowStart = preActLabelX + labelWidth + 8;
            const arrowEnd = arrowStart + 40;
            drawArrow(ctx, arrowStart, midY, arrowEnd, midY, diagramPalette.accentSoft);

            ctx.font = '14px ui-monospace, SFMono-Regular, Menlo, monospace';
            ctx.fillStyle = diagramPalette.text;
            ctx.fillText('tanh activation', arrowEnd + 10, midY + 5);

            ctx.textAlign = 'center';
            ctx.font = '14px ui-monospace, SFMono-Regular, Menlo, monospace';
            ctx.fillText('preActivation = Wx · xₜ  +  Wh · hₜ₋₁  +  bias', width / 2,
                frame.y + frame.height + 40);
        }

        function drawRnnTimelineDiagram() {
            const canvas = document.getElementById('rnnTimelineDiagram');
            if (!canvas) return;

            const { ctx, width, height } = setupCanvas(canvas);
            ctx.clearRect(0, 0, width, height);
            ctx.fillStyle = diagramPalette.background;
            ctx.fillRect(0, 0, width, height);

            const steps = [
                { time: 'Time 0', output: 'Output 0', hidden: 'h(0)' },
                { time: 'Time 1', output: 'Output 1', hidden: 'h(1)' },
                { time: 'Time 2', output: 'Output 2', hidden: 'h(2)' },
                { time: 'Time 3', output: 'Output 3', hidden: 'h(3)' }
            ];

            const boxWidth = 120;
            const boxHeight = 50;
            const gap = 60;
            const totalWidth = steps.length * boxWidth + (steps.length - 1) * gap;
            const startX = (width - totalWidth) / 2;
            const topY = 50;
            const bottomY = 160;

            steps.forEach((step, index) => {
                const x = startX + index * (boxWidth + gap);

                ctx.textAlign = 'center';
                ctx.fillStyle = diagramPalette.muted;
                ctx.font = '12px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
                ctx.fillText(step.time, x + boxWidth / 2, topY - 12);

                drawRoundedRect(ctx, x, topY, boxWidth, boxHeight, 10, diagramPalette.panel, diagramPalette.line);
                ctx.fillStyle = diagramPalette.text;
                ctx.font = '14px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
                ctx.fillText('Cell', x + boxWidth / 2, topY + 30);

                drawRoundedRect(ctx, x, bottomY, boxWidth, boxHeight, 10, '#171726', diagramPalette.line);
                ctx.fillStyle = diagramPalette.text;
                ctx.fillText('tanh', x + boxWidth / 2, bottomY + boxHeight / 2 + 5);

                drawArrow(ctx, x + boxWidth / 2, topY + boxHeight, x + boxWidth / 2, bottomY, diagramPalette.accent);

                drawArrow(ctx, x + boxWidth / 2, bottomY + boxHeight, x + boxWidth / 2, bottomY + boxHeight + 35, diagramPalette.accentSoft);
                ctx.fillStyle = diagramPalette.muted;
                ctx.font = '12px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
                ctx.fillText(step.output, x + boxWidth / 2, bottomY + boxHeight + 48);

                if (index === steps.length - 1) {
                    drawArrow(ctx, x + boxWidth, bottomY + boxHeight / 2, x + boxWidth + 50, bottomY + boxHeight / 2,
                        diagramPalette.accentSoft);
                    ctx.fillStyle = diagramPalette.text;
                    ctx.fillText(step.hidden, x + boxWidth + 25, bottomY + boxHeight / 2 - 12);
                }

                if (index < steps.length - 1) {
                    const nextX = startX + (index + 1) * (boxWidth + gap);
                    const startY = bottomY + boxHeight / 2;
                    const endY = bottomY + boxHeight / 2;
                    drawArrow(ctx, x + boxWidth, startY, nextX, endY);

                    ctx.fillStyle = diagramPalette.text;
                    ctx.font = '12px ui-sans-serif, system-ui, -apple-system, "Segoe UI", sans-serif';
                    ctx.fillText(step.hidden, (x + boxWidth + nextX) / 2, startY - 10);
                }
            });

        }
    </script>
</body>
</html>
